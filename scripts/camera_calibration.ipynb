{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "calib_id = \"07-31-25\"\n",
    "calib_base_folder = f\"/home/arclab/catkin_ws/src/Catheter-Perception/camera_calibration/{calib_id}\"\n",
    "# calib_base_folder = f\"C:\\\\Users\\\\jlim\\\\Documents\\\\GitHub\\\\Catheter-Perception\\\\camera_calibration\\\\{calib_id}\"\n",
    "# Camera 0 refers to top camera, Camera 1 refers to bottom camera\n",
    "# Ensure camera port ids are correct\n",
    "# port_ids = [0, 2]\n",
    "cam0_device = \"/dev/cam0\"\n",
    "cam1_device = \"/dev/cam1\"\n",
    "cam2_device = \"/dev/cam2\"\n",
    "devices = [f\"/dev/cam{i}\" for i in range(3)]\n",
    "cap_frame_width = 1280\n",
    "cap_frame_height = 720\n",
    "\n",
    "# Ensure proper camera configurations\n",
    "focus_values = [35, 75, 75]  # Default focus values for cam0, cam1, cam2\n",
    "config_commands = {cam0_device: [\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_absolute={focus_values[0]}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ],\n",
    "                cam1_device: [\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_absolute={focus_values[1]}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ],\n",
    "                cam2_device: [\n",
    "                    f\"v4l2-ctl -d {cam2_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam2_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam2_device} -c focus_absolute={focus_values[2]}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "def configure_camera(devices, config_commands):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        for command in config_commands[device]:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Camera Focus Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Configure and open cameras to check proper configs\n",
    "d=0\n",
    "cap = cv2.VideoCapture(devices[d], cv2.CAP_V4L2)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_frame_height)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: One or both cameras could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "focus_adjust = focus_values[d]\n",
    "configure_wait_time = 2 # wait this many seconds to configure manual settings\n",
    "configure_yet = False\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    if time.time() - start_time > configure_wait_time and not configure_yet:\n",
    "        configure_camera([devices[d]], config_commands)\n",
    "        configure_yet = True\n",
    "        print(\"Configured manual camera settings!!!\")\n",
    "\n",
    "    # Read frames from both cameras\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: One or both frames could not be read.\")\n",
    "        break\n",
    "    cv2.putText(frame, f\"Focus: {focus_adjust}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(f\"Camera {d}\", frame)\n",
    "\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord('q'):  # Increase focus for Camera 0\n",
    "        focus_adjust = min(focus_adjust + 5, 250)\n",
    "        command = f\"v4l2-ctl -d {devices[d]} -c focus_absolute={focus_adjust}\"\n",
    "        subprocess.run(command, shell=True)\n",
    "        print(f\"Cam {d} focus set to: {focus_adjust}\")\n",
    "    elif key == ord('a'):  # Decrease focus for Camera 0\n",
    "        focus_adjust = max(focus_adjust - 5, 0)\n",
    "        command = f\"v4l2-ctl -d {devices[d]} -c focus_absolute={focus_adjust}\"\n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "# Release both cameras and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Camera configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Configure and open cameras to check proper configs\n",
    "caps = []\n",
    "for device in devices:\n",
    "    cap = cv2.VideoCapture(device, cv2.CAP_V4L2)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_frame_width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_frame_height)\n",
    "    caps.append(cap)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Camera {device} could not be opened.\")\n",
    "        exit()\n",
    "\n",
    "configure_wait_time = 2 # wait this many seconds to configure manual settings\n",
    "configure_yet = False\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    if time.time() - start_time > configure_wait_time and not configure_yet:\n",
    "        configure_camera([cam0_device, cam1_device], config_commands)\n",
    "        configure_yet = True\n",
    "        print(\"Configured manual camera settings!!!\")\n",
    "\n",
    "    # Read frames from both cameras\n",
    "    for i, cap in enumerate(caps):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Frame could not be read from camera.\")\n",
    "            exit()\n",
    "        cv2.imshow(f\"Camera {i} (Device: {devices[i]})\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord('q'):  # Increase focus for Camera 0\n",
    "        cam0 = min(cam0 + 5, 250)\n",
    "        command = f\"v4l2-ctl -d {cam0_device} -c focus_absolute={cam0_focus_adjust}\"\n",
    "        subprocess.run(command, shell=True)\n",
    "        print(f\"Cam 0 focus set to: {cam0_focus_adjust}\")\n",
    "    elif key == ord('a'):  # Decrease focus for Camera 0\n",
    "        cam0_focus_adjust = max(cam0_focus_adjust - 5, 0)\n",
    "        command = f\"v4l2-ctl -d {cam0_device} -c focus_absolute={cam0_focus_adjust}\"\n",
    "        subprocess.run(command, shell=True)\n",
    "        print(f\"Cam 0 focus set to: {cam0_focus_adjust}\")\n",
    "    elif key == ord('w'): # Increase focus for Camera 1\n",
    "        cam1_focus_adjust = min(cam1_focus_adjust + 5, 250)\n",
    "        command = f\"v4l2-ctl -d {cam1_device} -c focus_absolute={cam1_focus_adjust}\"\n",
    "        subprocess.run(command, shell=True)\n",
    "        print(f\"Cam 1 focus set to: {cam1_focus_adjust}\")\n",
    "    elif key == ord('s'):\n",
    "        # Decrease focus for Camera 1\n",
    "        cam1_focus_adjust = max(cam1_focus_adjust - 5, 0)\n",
    "        command = f\"v4l2-ctl -d {cam1_device} -c focus_absolute={cam1_focus_adjust}\"\n",
    "        subprocess.run(command, shell=True)\n",
    "        print(f\"Cam 1 focus set to: {cam1_focus_adjust}\")\n",
    "\n",
    "# Release both cameras and close windows\n",
    "for cap in caps:\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACE to freeze frame and select ROI.\n",
      "Draw the box, then press ENTER to confirm or ESC to cancel.\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "ROI Coordinates: x=152, y=119, w=328, h=176\n",
      "(152, 119, 328, 176)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "roi_boxes = []\n",
    "for d in range(len(devices)):\n",
    "    cap = cv2.VideoCapture(devices[d], cv2.CAP_V4L2)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_frame_width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_frame_height)\n",
    "\n",
    "    print(\"Press SPACE to freeze frame and select ROI.\")\n",
    "    print(\"Draw the box, then press ENTER to confirm or ESC to cancel.\")\n",
    "\n",
    "    roi_box = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Live Feed\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == 32:  # SPACE to pause and select ROI\n",
    "            cv2.imshow(\"Select ROI\", frame)\n",
    "            roi_box = cv2.selectROI(\"Select ROI\", frame, showCrosshair=True, fromCenter=False)\n",
    "            cv2.destroyWindow(\"Select ROI\")\n",
    "\n",
    "            if roi_box != (0, 0, 0, 0):\n",
    "                x, y, w, h = roi_box\n",
    "                cropped = frame[y:y+h, x:x+w]\n",
    "                cv2.imshow(\"Cropped ROI\", cropped)\n",
    "                print(f\"ROI Coordinates: x={x}, y={y}, w={w}, h={h}\")\n",
    "            else:\n",
    "                print(\"No ROI selected.\")\n",
    "\n",
    "        elif key == 27:  # ESC to exit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Final ROI for Camera {d}: {roi_box}\")\n",
    "    roi_boxes.append(roi_box)\n",
    "\n",
    "# Save the ROI coordinates to a file\n",
    "roi_file_path = os.path.join(calib_base_folder, f\"roi_cam{d}.txt\")\n",
    "with open(roi_file_path, 'w') as f:\n",
    "    for box in roi_boxes:\n",
    "        f.write(f\"{box[0]} {box[1]} {box[2]} {box[3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Intrinsic Calibration Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "devices = [f\"/dev/cam{i}\" for i in [0, 1, 2]]\n",
    "\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"Collecting instrinsic calibration images for camera {i}...\")\n",
    "    output_dir = f\"{calib_base_folder}/charuco_calib_images/cam{i}\"  # Directory to save images\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(device, cv2.CAP_V4L2)\n",
    "\n",
    "    # Check if camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"Press SPACE to capture image, ESC to exit.\")\n",
    "\n",
    "    image_count = 0\n",
    "    configure_wait_time = 2 # wait this many seconds to configure manual settings\n",
    "    configure_yet = False\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        if time.time() - start_time > configure_wait_time and not configure_yet:\n",
    "            configure_camera([device], config_commands)\n",
    "            configure_yet = True\n",
    "            print(\"Configured manual camera settings!!!\")\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "\n",
    "        # Display the live feed\n",
    "        cv2.imshow(\"Camera Feed - Press SPACE to capture, ESC to exit\", frame)\n",
    "\n",
    "        # Keyboard input\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Save image on SPACE key press\n",
    "        if key == ord(' '):\n",
    "            # Construct image filename\n",
    "            image_path = os.path.join(output_dir, f\"img_{image_count:03d}.png\")\n",
    "            # Save the frame as PNG\n",
    "            cv2.imwrite(image_path, frame)\n",
    "            print(f\"Captured: {image_path}\")\n",
    "            image_count += 1\n",
    "\n",
    "        # Exit on ESC key press\n",
    "        elif key == 27:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "    # Release the camera and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Stereo Calibration Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect images of calibration board in both cameras frames for stereo extrinsic calibration\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "device_ids = [0, 1]\n",
    "cur_devices = [f\"/dev/cam{i}\" for i in device_ids]\n",
    "output_dir = f\"{calib_base_folder}/stereo_calib_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "caps = []\n",
    "for device in cur_devices:\n",
    "    cap = cv2.VideoCapture(device, cv2.CAP_V4L2)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Camera {device} could not be opened.\")\n",
    "        exit()\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_frame_width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, cap_frame_height)\n",
    "    caps.append(cap)\n",
    "\n",
    "configure_wait_time = 2 # wait this many seconds to configure manual settings\n",
    "configure_yet = False\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    if time.time() - start_time > configure_wait_time and not configure_yet:\n",
    "        configure_camera(cur_devices, config_commands)\n",
    "        configure_yet = True\n",
    "        print(\"Configured manual camera settings!!!\")\n",
    "\n",
    "    # Read frames from both cameras\n",
    "    frames = []\n",
    "    for cap in caps:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Frame could not be read from camera.\")\n",
    "            exit()\n",
    "        # resize for better display\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Combine and display both frames\n",
    "    combined = cv2.hconcat(frames)\n",
    "    cv2.imshow(\"Camera 0 (top) + Camera 1 (side)\", combined)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord(' '):  # Space key to capture images\n",
    "        # Save images with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        for i, d in enumerate(device_ids):\n",
    "            img_path = f\"{output_dir}/cam{d}_{frame_count}.png\"\n",
    "            cv2.imwrite(img_path, frames[i])\n",
    "            print(f\"Captured images:\\n - {img_path}\\n\")\n",
    "        frame_count += 1\n",
    "\n",
    "# Release both cameras and close windows\n",
    "for cap in caps:\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Camera-World Calibration Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect images for finding camera-world transform\n",
    "# Current approach is to use aruco marker to find camera 0 pose wrt to aruco marker\n",
    "# Then camera-camera transform can be used to find camera 1 to world transform\n",
    "import time\n",
    "\n",
    "# Make sure cameras are configured\n",
    "# Open cameras with higher resolution\n",
    "cap0 = cv2.VideoCapture(cam0_device, cv2.CAP_V4L2)\n",
    "cap1 = cv2.VideoCapture(cam1_device, cv2.CAP_V4L2)\n",
    "cap0.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap0.set(cv2.CAP_PROP_FRAME_HEIGHT, 920)\n",
    "cap1.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 920)\n",
    "\n",
    "if not cap0.isOpened() or not cap1.isOpened():\n",
    "    print(\"Error: One or both cameras could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "output_dir = f\"{calib_base_folder}/extrinsic_calib_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "configure_wait_time = 2 # wait this many seconds to configure manual settings\n",
    "configure_yet = False\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "while True:\n",
    "\n",
    "    if time.time() - start_time > configure_wait_time and not configure_yet:\n",
    "        configure_camera([cam0_device, cam1_device], config_commands)\n",
    "        configure_yet = True\n",
    "        print(\"Configured manual camera settings!!!\")\n",
    "\n",
    "    # Read frame from camera\n",
    "    ret0, frame0 = cap0.read()\n",
    "    ret1, frame1 = cap1.read()\n",
    "\n",
    "    if not ret0 or not ret1:\n",
    "        print(\"Error: Frame could not be read.\")\n",
    "        break\n",
    "\n",
    "    # Combine and display both frames\n",
    "    combined = cv2.hconcat([frame0, frame1])\n",
    "    cv2.imshow(\"Camera 0 (top) + Camera 1 (side)\", combined)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord(' '):  # Space key to capture images\n",
    "        # Save images with timestamp\n",
    "        img0_path = f\"{output_dir}/cam0_{frame_count}.png\"\n",
    "        img1_path = f\"{output_dir}/cam1_{frame_count}.png\"\n",
    "        cv2.imwrite(img0_path, frame0)\n",
    "        cv2.imwrite(img1_path, frame1)\n",
    "        print(f\"Captured images:\\n - {img0_path}\\n - {img1_path}\")\n",
    "        frame_count += 1\n",
    "\n",
    "# Release both cameras and close windows\n",
    "cap0.release()\n",
    "cap1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Intrinsics Charuco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calib_base_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Calibrate instrinsics\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     intrinsic_calib_images_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalib_base_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/charuco_calib_images/cam\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m     rms_error, K, d, _, _ \u001b[38;5;241m=\u001b[39m calibrate_intrinsics_charuco(\n\u001b[0;32m     83\u001b[0m         intrinsic_calib_images_path, charuco_board, aruco_dict\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharuco intrinsics for camera \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RMS error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrms_error\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calib_base_folder' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute intrinsic, stereo extrinsic, and camera to world transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def calibrate_intrinsics_charuco(glob_pattern, charuco_board, aruco_dict,\n",
    "                                 min_markers=20):\n",
    "    '''\n",
    "    Calibrates camera intrinsics using a ChArUco board.\n",
    "    '''\n",
    "    all_corners, all_ids, img_size = [], [], None\n",
    "\n",
    "    for fname in glob.glob(glob_pattern):\n",
    "        img = cv2.imread(fname)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        if img_size is None:\n",
    "            img_size = img_gray.shape[::-1]\n",
    "\n",
    "        detector = aruco.CharucoDetector(charuco_board)\n",
    "        charuco_corners, charuco_ids, marker_corners, marker_ids = detector.detectBoard(img_gray)\n",
    "\n",
    "        all_corners.append(charuco_corners)\n",
    "        all_ids.append(charuco_ids)\n",
    "\n",
    "    # 3. Calibrate\n",
    "    ret, K, dist, rvecs, tvecs = aruco.calibrateCameraCharuco(\n",
    "        charucoCorners=all_corners,\n",
    "        charucoIds=all_ids,\n",
    "        board=charuco_board,\n",
    "        imageSize=img_size,\n",
    "        cameraMatrix=None,\n",
    "        distCoeffs=None\n",
    "    )\n",
    "    return ret, K, dist, rvecs, tvecs\n",
    "\n",
    "# Dict holding all camera calibration data\n",
    "camera_calib_data = {\n",
    "        \"cam0\": {\n",
    "            \"intrinsics\": {\n",
    "                \"K\": None,\n",
    "                \"d\": None\n",
    "            },\n",
    "            \"extrinsics\": { # this is camera to world transform\n",
    "                \"R\": None,\n",
    "                \"T\": None\n",
    "            }\n",
    "        },\n",
    "        \"cam1\": {\n",
    "            \"intrinsics\": {\n",
    "                \"K\": None,\n",
    "                \"d\": None\n",
    "            },\n",
    "            \"extrinsics\": {\n",
    "                \"R\": None,\n",
    "                \"T\": None\n",
    "            }\n",
    "        },\n",
    "        \"stereo_extrinsics\": { # Camera to camera transform (from cam0 to cam1)\n",
    "            \"R_1_0\": None,\n",
    "            \"T_1_0\": None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Define ChArUco board used\n",
    "square_size = 0.006\n",
    "marker_length = 0.004\n",
    "board_size = (10, 10)\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "charuco_board = aruco.CharucoBoard(\n",
    "    size=board_size,\n",
    "    squareLength=square_size, \n",
    "    markerLength=marker_length,\n",
    "    dictionary=aruco_dict\n",
    ")\n",
    "\n",
    "# Calibrate instrinsics\n",
    "for i in range(2):\n",
    "    intrinsic_calib_images_path = f\"{calib_base_folder}/charuco_calib_images/cam{i}/*.png\"\n",
    "    rms_error, K, d, _, _ = calibrate_intrinsics_charuco(\n",
    "        intrinsic_calib_images_path, charuco_board, aruco_dict\n",
    "    )\n",
    "    print(f\"Charuco intrinsics for camera {i} RMS error: {rms_error:.4f}\")\n",
    "    print(f\"Camera {i} intrinsic matrix:\\n{K}\")\n",
    "    print(f\"Camera {i} distortion coefficients:\\n{d}\")\n",
    "    camera_calib_data[f\"cam{i}\"][\"intrinsics\"][\"K\"] = K\n",
    "    camera_calib_data[f\"cam{i}\"][\"intrinsics\"][\"d\"] = d\n",
    "\n",
    "print(\"Intrinsic calibration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Stereo Extrinsics Charuco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def detect_charuco_corners(image_path, board, detector):\n",
    "    \"\"\"Detects Charuco corners using OpenCV 4.11+ functions.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Aruco markers and Charuco corners\n",
    "    charuco_corners, charuco_ids, _, _ = detector.detectBoard(gray)\n",
    "\n",
    "    if charuco_ids is not None and len(charuco_ids) > 4:\n",
    "        return charuco_corners, charuco_ids\n",
    "    return None, None\n",
    "\n",
    "def calculate_relative_transform(R1, T1, R2, T2):\n",
    "    \"\"\"Calculate the relative rotation and translation between two cameras.\n",
    "    Returns R21 and T21, the rotation and translation of camera 2 in camera 1's frame.\"\"\"\n",
    "    R21 = np.dot(R2, R1.T)\n",
    "    T21 = T2 - np.dot(R21, T1)\n",
    "    return R21, T21\n",
    "\n",
    "def get_extrinsics(image_path, K, dist, board, detector):\n",
    "    \"\"\"Compute extrinsics for a single image.\"\"\"\n",
    "    charuco_corners, charuco_ids = detect_charuco_corners(image_path, board, detector)\n",
    "    if charuco_corners is not None:\n",
    "        obj_points = board.getChessboardCorners()[charuco_ids.flatten()]\n",
    "        ret, rvec, tvec = cv2.solvePnP(obj_points, charuco_corners, K, dist)\n",
    "        if ret:\n",
    "            R, _ = cv2.Rodrigues(rvec)\n",
    "            return R, tvec\n",
    "    return None, None\n",
    "\n",
    "# # Load intrinsic parameters\n",
    "# calib_data_file = \"../camera_calibration/05-08-25/camera_calib_data.pkl\"\n",
    "# with open(calib_data_file, 'rb') as f:\n",
    "#     calib_data = pickle.load(f)\n",
    "\n",
    "K0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "K1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "dist1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "\n",
    "# Define ChArUco board used\n",
    "square_size = 0.006\n",
    "marker_length = 0.004\n",
    "board_size = (10, 10)\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "charuco_board = aruco.CharucoBoard(\n",
    "    size=board_size,\n",
    "    squareLength=square_size, \n",
    "    markerLength=marker_length,\n",
    "    dictionary=aruco_dict\n",
    ")\n",
    "\n",
    "# Create the Aruco and Charuco detector objects (new in OpenCV 4.11+)\n",
    "aruco_detector = aruco.ArucoDetector(aruco_dict)\n",
    "charuco_detector = aruco.CharucoDetector(charuco_board)\n",
    "\n",
    "# Paths to synchronized images\n",
    "cam0_images = sorted(glob.glob(f\"{calib_base_folder}/stereo_calib_images/cam0_*.png\"))\n",
    "cam1_images = sorted(glob.glob(f\"{calib_base_folder}/stereo_calib_images/cam1_*.png\"))\n",
    "\n",
    "# Arrays to store transformations\n",
    "relative_rotations = []\n",
    "relative_translations = []\n",
    "\n",
    "for img0_path, img1_path in zip(cam0_images, cam1_images):\n",
    "    # Get extrinsics for both cameras using updated functions\n",
    "    R0, T0 = get_extrinsics(img0_path, K0, dist0, charuco_board, charuco_detector)\n",
    "    R1, T1 = get_extrinsics(img1_path, K1, dist1, charuco_board, charuco_detector)\n",
    "\n",
    "    if R0 is not None and R1 is not None:\n",
    "        # Calculate the relative transformation between cameras\n",
    "        R10, T10 = calculate_relative_transform(R0, T0, R1, T1)\n",
    "        relative_rotations.append(R10)\n",
    "        relative_translations.append(T10)\n",
    "\n",
    "# Average the transformations\n",
    "R_avg = sum(relative_rotations) / len(relative_rotations)\n",
    "T_avg = sum(relative_translations) / len(relative_translations)\n",
    "\n",
    "# Print the averaged relative transformation\n",
    "print(\"Transform from Camera 0 to Camera 1:\")\n",
    "print(\"Averaged Relative Rotation (R10):\\n\", R_avg)\n",
    "print(\"Averaged Relative Translation (T10):\\n\", T_avg)\n",
    "\n",
    "# Save the relative transformation\n",
    "camera_calib_data[\"stereo_extrinsics\"][\"R_1_0\"] = R_avg\n",
    "camera_calib_data[\"stereo_extrinsics\"][\"T_1_0\"] = T_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Camera-World Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find camera to world using PnP and calibrator piece\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "img_dir = \"../camera_calibration/05-16-25/extrinsic_calib_images\"\n",
    "img0 = cv2.imread(os.path.join(img_dir, \"cam0_0.png\"))\n",
    "img1 = cv2.imread(os.path.join(img_dir, \"cam1_0.png\"))\n",
    "# Downsize images for display\n",
    "img0 = cv2.resize(img0, (640, 480))\n",
    "img1 = cv2.resize(img1, (640, 480))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click points in both images. Press any key to finish and close windows.\n",
      "Points in img0: []\n",
      "Points in img1: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lists to store clicked points\n",
    "points_img0 = []\n",
    "points_img1 = []\n",
    "\n",
    "def click_event_img0(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        points_img0.append((x, y))\n",
    "        print(f\"Image 0: Point {len(points_img0)}: ({x}, {y})\")\n",
    "        cv2.circle(img0, (x, y), 5, (0, 0, 255), -1)\n",
    "        cv2.imshow(\"Cam0\", img0)\n",
    "\n",
    "def click_event_img1(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        points_img1.append((x, y))\n",
    "        print(f\"Image 1: Point {len(points_img1)}: ({x}, {y})\")\n",
    "        cv2.circle(img1, (x, y), 5, (0, 0, 255), -1)\n",
    "        cv2.imshow(\"Cam1\", img1)\n",
    "\n",
    "cv2.imshow(\"Cam0\", img0)\n",
    "cv2.setMouseCallback(\"Cam0\", click_event_img0)\n",
    "cv2.imshow(\"Cam1\", img1)\n",
    "cv2.setMouseCallback(\"Cam1\", click_event_img1)\n",
    "\n",
    "print(\"Click points in both images. Press any key to finish and close windows.\")\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Points in img0:\", points_img0)\n",
    "print(\"Points in img1:\", points_img1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read in camera intrinsics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../camera_calibration/05-08-25/camera_calib_data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     camera_calib_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m K0 \u001b[38;5;241m=\u001b[39m camera_calib_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcam0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintrinsics\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m dist0 \u001b[38;5;241m=\u001b[39m camera_calib_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcam0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintrinsics\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Read in camera intrinsics\n",
    "with open(\"../camera_calibration/05-08-25/camera_calib_data.pkl\", 'rb') as f:\n",
    "    camera_calib_data = pickle.load(f)\n",
    "K0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "K1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "dist1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "\n",
    "# Defined corresponding world frame coordinates of clicked points\n",
    "# These should be in the same order as the clicked points\n",
    "world_coords_0 = np.array([[0, 4.125, 0],\n",
    "                [0, 16, 9],\n",
    "                [0, 0, 23.95],\n",
    "                [-4.125, 0, 0],\n",
    "                [-16, 0, 9],\n",
    "                [4.125, 0, 0],\n",
    "                [16, 0, 9]]) / 1000\n",
    "\n",
    "world_coords_1 = np.array([[0, 4.125, 0],\n",
    "                [0, 16, 9],\n",
    "                [0, 0, 23.95],\n",
    "                [-4.125, 0, 0],\n",
    "                [-16, 0, 9],\n",
    "                [0, -4.125, 0],\n",
    "                [0, -16, 9]]) / 1000\n",
    "\n",
    "# Find world frame transform for each camera using solvePnP\n",
    "def find_camera_to_world_transform(points_img, world_coords, K, dist):\n",
    "    # Convert points to numpy arrays\n",
    "    points_img = np.array(points_img, dtype=np.float32)\n",
    "    world_coords = np.array(world_coords, dtype=np.float32)\n",
    "\n",
    "    # Solve PnP\n",
    "    _, rvec, tvec = cv2.solvePnP(world_coords, points_img, K, dist)\n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    return R, tvec\n",
    "\n",
    "R0, T0 = find_camera_to_world_transform(points_img0, world_coords_0, K0, dist0)\n",
    "R1, T1 = find_camera_to_world_transform(points_img1, world_coords_1, K1, dist1)\n",
    "print(\"Camera 0 to world transform:\")\n",
    "print(\"Rotation:\\n\", R0)\n",
    "print(\"Translation:\\n\", T0)\n",
    "print(\"Camera 1 to world transform:\")\n",
    "print(\"Rotation:\\n\", R1)\n",
    "print(\"Translation:\\n\", T1)\n",
    "\n",
    "# Visualize world frame axes in each camera frame\n",
    "def draw_axes(image, R, T, K, dist, axis_length=0.02):\n",
    "    \"\"\"Draw the world frame axes on the image.\"\"\"\n",
    "    axis_points = np.float32([\n",
    "        [0, 0, 0],                  # Origin\n",
    "        [axis_length, 0, 0],        # X-axis (red)\n",
    "        [0, axis_length, 0],        # Y-axis (green)\n",
    "        [0, 0, axis_length]         # Z-axis (blue)\n",
    "    ]).reshape(-1, 3)\n",
    "    img_points, _ = cv2.projectPoints(axis_points, cv2.Rodrigues(R)[0], T, K, dist)\n",
    "    origin = tuple(img_points[0].ravel().astype(int))\n",
    "    cv2.line(image, origin, tuple(img_points[1].ravel().astype(int)), (0, 0, 255), 3)\n",
    "    cv2.line(image, origin, tuple(img_points[2].ravel().astype(int)), (0, 255, 0), 3)\n",
    "    cv2.line(image, origin, tuple(img_points[3].ravel().astype(int)), (255, 0, 0), 3)\n",
    "    cv2.putText(image, \"X\", tuple(img_points[1].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(image, \"Y\", tuple(img_points[2].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(image, \"Z\", tuple(img_points[3].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "# # Replace extrinsic calibration parameters\n",
    "# camera_calib_data[\"cam0\"][\"extrinsics\"][\"R\"] = R0\n",
    "# camera_calib_data[\"cam0\"][\"extrinsics\"][\"T\"] = T0\n",
    "# camera_calib_data[\"cam1\"][\"extrinsics\"][\"R\"] = R1\n",
    "# camera_calib_data[\"cam1\"][\"extrinsics\"][\"T\"] = T1\n",
    "# # Save the camera calibration data\n",
    "# with open(f\"../camera_calibration/05-16-25/camera_calib_data.pkl\", 'wb') as f:\n",
    "#     pickle.dump(camera_calib_data, f)\n",
    "\n",
    "# Show camera 0 world frame transform in both camera frames\n",
    "# Compute Camera 1 to world transform using stereo extrinsics\n",
    "R_1_0 = camera_calib_data[\"stereo_extrinsics\"][\"R_1_0\"]\n",
    "T_1_0 = camera_calib_data[\"stereo_extrinsics\"][\"T_1_0\"]\n",
    "R1_from0 = np.dot(R_1_0, R0)\n",
    "T1_from0 = np.dot(R_1_0, T0) + T_1_0\n",
    "print(\"Camera 1 to world transform using stereo extrinsics:\")\n",
    "print(\"Rotation:\\n\", R1_from0)\n",
    "print(\"Translation:\\n\", T1_from0)\n",
    "# Show camera 1 world frame transform in both camera frames\n",
    "R0_from1 = np.dot(R_1_0.T, R1)\n",
    "T0_from1 = np.dot(R_1_0.T, T1) - T_1_0\n",
    "print(\"Camera 0 to world transform using stereo extrinsics:\")\n",
    "print(\"Rotation:\\n\", R0_from1)\n",
    "print(\"Translation:\\n\", T0_from1)\n",
    "\n",
    "# Draw axes on img0 and img1\n",
    "# draw_axes(img0, R0, T0, K0, dist0)\n",
    "draw_axes(img0, R0_from1, T0_from1, K0, dist0)\n",
    "cv2.imshow(\"Camera 0 - World Frame Axes from cam 0 transform\", img0)\n",
    "# cv2.imshow(\"Camera 1 - World Frame Axes from cam 0 transform\", img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Draw axes on img0 and img1\n",
    "# draw_axes(img1, R1_from0, T1_from0, K1, dist1)\n",
    "draw_axes(img1, R1, T1, K1, dist1)\n",
    "# cv2.imshow(\"Camera 0 - World Frame Axes from cam 1 transform\", img0)\n",
    "cv2.imshow(\"Camera 1 - World Frame Axes from cam 1 transform\", img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Transformation:\n",
      " [[ 0.00558221  0.99967035  0.02471735  0.02573521]\n",
      " [-0.99877942  0.00438786  0.04866734  0.44270566]\n",
      " [ 0.04854917 -0.02497695  0.99848393  0.03046309]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Deviation from Identity: 2.044639980180462\n",
      "The transformations are not equivalent.\n",
      "Relative Transformation:\n",
      " [[-0.98008459 -0.19790326  0.01473119  0.45478905]\n",
      " [ 0.19204885 -0.96450462 -0.18119603 -0.42941468]\n",
      " [ 0.05005939 -0.17474519  0.98331469 -0.04832565]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Deviation from Identity: 2.8837469498880535\n",
      "The transformations are not equivalent.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_matrix(R, t):\n",
    "    \"\"\"Construct a homogeneous transformation matrix from rotation and translation.\"\"\"\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t\n",
    "    return T\n",
    "\n",
    "def check_transform_equivalence(R1, t1, R2, t2):\n",
    "    \"\"\"Check if two transformations are equivalent.\"\"\"\n",
    "    # Construct transformation matrices\n",
    "    T1 = transform_matrix(R1, t1)\n",
    "    T2 = transform_matrix(R2, t2)\n",
    "    \n",
    "    # Compute the relative transformation\n",
    "    relative_transform = np.linalg.inv(T1) @ T2\n",
    "    identity_matrix = np.eye(4)\n",
    "    \n",
    "    # Compute the deviation from the identity matrix\n",
    "    deviation = np.linalg.norm(relative_transform - identity_matrix)\n",
    "\n",
    "    # Print the relative transformation and the deviation\n",
    "    print(\"Relative Transformation:\\n\", relative_transform)\n",
    "    print(\"Deviation from Identity:\", deviation)\n",
    "\n",
    "    # Threshold for considering them equivalent\n",
    "    tolerance = 1e-6\n",
    "    if deviation < tolerance:\n",
    "        print(\"The transformations are approximately equivalent.\")\n",
    "    else:\n",
    "        print(\"The transformations are not equivalent.\")\n",
    "\n",
    "# # Example usage\n",
    "# R1 = np.array([[-0.996, -0.087, 0.012],\n",
    "#                [-0.032, 0.234, -0.972],\n",
    "#                [0.082, -0.968, -0.236]])\n",
    "\n",
    "# t1 = np.array([-0.003, 0.045, -0.221])\n",
    "\n",
    "# R2 = np.array([[-0.104, -0.994, 0.029],\n",
    "#                [-0.080, -0.020, -0.997],\n",
    "#                [0.991, -0.106, -0.078]])\n",
    "\n",
    "# t2 = np.array([0.015, 0.028, -0.273])\n",
    "\n",
    "check_transform_equivalence(R1, T0.flatten(), R0_from1, T0_from1.flatten())\n",
    "check_transform_equivalence(R1, T1.flatten(), R1_from0, T1_from0.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined R_0_w:\n",
      " [[-0.72882985  0.22494091  0.64669053]\n",
      " [-0.67096253 -0.42285115 -0.60910277]\n",
      " [ 0.1364417  -0.87783739  0.45911347]]\n",
      "Refined T_0_W:\n",
      " [ 0.21338508 -0.74842295  0.92909777]\n",
      "Refined R_1_w:\n",
      " [[ 0.63000373  0.2387844   0.73897044]\n",
      " [-0.58767338 -0.47546573  0.65465436]\n",
      " [ 0.50767637 -0.84670795 -0.15921795]]\n",
      "Refined T_1_W:\n",
      " [ 0.23905185 -1.00647314  1.04948871]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "import pdb\n",
    "\n",
    "### Currently does not work\n",
    "\n",
    "def rotation_matrix_to_vector(R):\n",
    "    \"\"\"Convert a rotation matrix to a rotation vector using Rodrigues' formula.\"\"\"\n",
    "    rvec, _ = cv2.Rodrigues(R)\n",
    "    return rvec.flatten()\n",
    "\n",
    "\n",
    "def vector_to_rotation_matrix(rvec):\n",
    "    \"\"\"Convert a rotation vector to a rotation matrix using Rodrigues' formula.\"\"\"\n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    return R\n",
    "\n",
    "\n",
    "def transform_matrix(R, t):\n",
    "    \"\"\"Construct a homogeneous transformation matrix from rotation and translation.\"\"\"\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t\n",
    "    return T\n",
    "\n",
    "\n",
    "def decompose_transform(T):\n",
    "    \"\"\"Decompose a transformation matrix into rotation and translation.\"\"\"\n",
    "    R = T[:3, :3]\n",
    "    t = T[:3, 3]\n",
    "    return R, t\n",
    "\n",
    "\n",
    "def cost_function(params, R_0_w, T_0_W, R_1_0, T_1_0, R_1_w, T_1_W):\n",
    "    \"\"\"Cost function to minimize the difference between expected and actual transforms.\"\"\"\n",
    "    rvec0, rvec1, t0, t1 = np.split(params, [3, 6, 9])\n",
    "    R0 = vector_to_rotation_matrix(rvec0)\n",
    "    R1 = vector_to_rotation_matrix(rvec1)\n",
    "    T0 = transform_matrix(R0, t0)\n",
    "    T1 = transform_matrix(R1, t1)\n",
    "    expected_T1 = T0 @ transform_matrix(R_1_0, T_1_0)\n",
    "    residual = (T1 - expected_T1).flatten()\n",
    "    return residual\n",
    "\n",
    "\n",
    "def refine_transforms(R_0_w, T_0_W, R_1_0, T_1_0, R_1_w, T_1_W):\n",
    "    \"\"\"Optimize transforms to improve consistency.\"\"\"\n",
    "    rvec0 = rotation_matrix_to_vector(R_0_w)\n",
    "    rvec1 = rotation_matrix_to_vector(R_1_w)\n",
    "    # pdb.set_trace()\n",
    "    initial_params = np.hstack([rvec0, rvec1, T_0_W, T_1_W])\n",
    "    result = least_squares(cost_function, initial_params, args=(R_0_w, T_0_W, R_1_0, T_1_0, R_1_w, T_1_W))\n",
    "    rvec0_opt, rvec1_opt, t0_opt, t1_opt = np.split(result.x, [3, 6, 9])\n",
    "    R0_opt = vector_to_rotation_matrix(rvec0_opt)\n",
    "    R1_opt = vector_to_rotation_matrix(rvec1_opt)\n",
    "    return R0_opt, t0_opt, R1_opt, t1_opt\n",
    "\n",
    "# Load the camera calibration data\n",
    "with open(f\"../camera_calibration/05-16-25/camera_calib_data.pkl\", 'rb') as f:\n",
    "    camera_calib_data = pickle.load(f)\n",
    "\n",
    "R_0_w = camera_calib_data[\"cam0\"][\"extrinsics\"][\"R\"]\n",
    "T_0_W = camera_calib_data[\"cam0\"][\"extrinsics\"][\"T\"].flatten()\n",
    "R_1_0 = camera_calib_data[\"stereo_extrinsics\"][\"R_1_0\"]\n",
    "T_1_0 = camera_calib_data[\"stereo_extrinsics\"][\"T_1_0\"].flatten()\n",
    "R_1_w = camera_calib_data[\"cam1\"][\"extrinsics\"][\"R\"]\n",
    "T_1_W = camera_calib_data[\"cam1\"][\"extrinsics\"][\"T\"].flatten()\n",
    "# Refine the transforms\n",
    "R0_refined, T0_refined, R1_refined, T1_refined = refine_transforms(R_0_w, T_0_W, R_1_0, T_1_0, R_1_w, T_1_W)\n",
    "print(\"Refined R_0_w:\\n\", R0_refined)\n",
    "print(\"Refined T_0_W:\\n\", T0_refined)\n",
    "print(\"Refined R_1_w:\\n\", R1_refined)\n",
    "print(\"Refined T_1_W:\\n\", T1_refined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aruco marker world frame calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[295., 147.],\n",
      "        [307., 146.],\n",
      "        [308., 159.],\n",
      "        [296., 159.]]], dtype=float32),)\n",
      "Object points:\n",
      " [[0.    0.    0.   ]\n",
      " [0.004 0.    0.   ]\n",
      " [0.004 0.004 0.   ]\n",
      " [0.    0.004 0.   ]]\n",
      "Camera 0 to World Rotation:\n",
      "[[ 0.96241617  0.07780901  0.26019391]\n",
      " [ 0.002285    0.95572452 -0.29425401]\n",
      " [-0.27156931  0.28378936  0.91962694]]\n",
      "Camera 0 to World Translation:\n",
      "[[-0.00391395]\n",
      " [-0.03319903]\n",
      " [ 0.20169472]]\n",
      "Camera 1 to World Rotation:\n",
      "[[ 0.27576602 -0.25516868 -0.92672725]\n",
      " [-0.05900692  0.95778878 -0.28126257]\n",
      " [ 0.95939296  0.13223629  0.24905744]]\n",
      "Camera 1 to World Translation:\n",
      "[[-0.03173681]\n",
      " [-0.01782421]\n",
      " [ 0.22452266]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def solve_pnp(marker_corners, marker_length, K, dist):\n",
    "    \"\"\"Solve PnP for a single Aruco marker.\"\"\"\n",
    "    half_length = marker_length / 2\n",
    "\n",
    "    # 3D points of the marker corners in the world coordinate system (origin at one corner)\n",
    "    obj_points = np.array([\n",
    "        [0, 0, 0],                  # Bottom-left (origin)\n",
    "        [marker_length, 0, 0],      # Bottom-right (X-axis)\n",
    "        [marker_length, marker_length, 0], # Top-right\n",
    "        [0, marker_length, 0]       # Top-left (Z-axis)\n",
    "    ], dtype=np.float32)\n",
    "    print(\"Object points:\\n\", obj_points)\n",
    "    # Solve PnP to find rotation and translation\n",
    "    ret, rvec, tvec = cv2.solvePnP(obj_points, marker_corners, K, dist)\n",
    "    if ret:\n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "        return R, tvec\n",
    "    return None, None\n",
    "\n",
    "def draw_axes(image, R, T, K, dist):\n",
    "    \"\"\"Draw the world frame origin and axes on the image.\"\"\"\n",
    "    axis_length = 0.05  # 5 cm\n",
    "\n",
    "    # Define the 3D points for the axes\n",
    "    axis_points = np.float32([\n",
    "        [0, 0, 0],              # Origin\n",
    "        [axis_length, 0, 0],    # X-axis (red)\n",
    "        [0, axis_length, 0],    # Y-axis (green)\n",
    "        [0, 0, axis_length]     # Z-axis (blue)\n",
    "    ]).reshape(-1, 3)\n",
    "\n",
    "    # Project the 3D axis points onto the image plane\n",
    "    img_points, _ = cv2.projectPoints(axis_points, cv2.Rodrigues(R)[0], T, K, dist)\n",
    "\n",
    "    # Draw the origin point\n",
    "    origin = tuple(img_points[0].ravel().astype(int))\n",
    "    cv2.circle(image, origin, 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw the X, Y, Z axes\n",
    "    cv2.line(image, origin, tuple(img_points[1].ravel().astype(int)), (0, 0, 255), 3)  # X-axis (red)\n",
    "    cv2.line(image, origin, tuple(img_points[2].ravel().astype(int)), (0, 255, 0), 3)  # Y-axis (green)\n",
    "    cv2.line(image, origin, tuple(img_points[3].ravel().astype(int)), (255, 0, 0), 3)  # Z-axis (blue)\n",
    "\n",
    "    # Annotate the axes\n",
    "    cv2.putText(image, \"X\", tuple(img_points[1].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(image, \"Y\", tuple(img_points[2].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(image, \"Z\", tuple(img_points[3].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "# Load intrinsic calibration data\n",
    "# # Load intrinsic parameters\n",
    "calib_data_file = f\"../camera_calibration/05-08-25/camera_calib_data.pkl\"\n",
    "with open(calib_data_file, 'rb') as f:\n",
    "    camera_calib_data = pickle.load(f)\n",
    "\n",
    "K = camera_calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist = camera_calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "\n",
    "\n",
    "# Aruco marker parameters\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "marker_length = 0.004  # Length of the marker side in meters\n",
    "\n",
    "# Initialize the Aruco detector\n",
    "detector = aruco.ArucoDetector(aruco_dict)\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(f\"../camera_calibration/05-08-25/extrinsic_calib_images/cam0/img_aruco.png\")\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect the Aruco marker\n",
    "corners, ids, _ = detector.detectMarkers(img_gray)\n",
    "print(corners)\n",
    "\n",
    "if ids is not None:\n",
    "    R_0_w, T_0_w = solve_pnp(corners[0], marker_length, K, dist)\n",
    "    if R_0_w is not None:\n",
    "        print(f\"Camera 0 to World Rotation:\\n{R_0_w}\")\n",
    "        print(f\"Camera 0 to World Translation:\\n{T_0_w}\")\n",
    "\n",
    "        # Draw the marker and the world frame\n",
    "        aruco.drawDetectedMarkers(img, corners, ids)\n",
    "        draw_axes(img, R_0_w, T_0_w, K, dist)\n",
    "\n",
    "# Compute Camera 1 to world transform using stereo extrinsics\n",
    "R_1_0 = camera_calib_data[\"stereo_extrinsics\"][\"R_1_0\"]\n",
    "T_1_0 = camera_calib_data[\"stereo_extrinsics\"][\"T_1_0\"]\n",
    "R_1_w = np.dot(R_1_0, R_0_w)\n",
    "T_1_w = np.dot(R_1_0, T_0_w) + T_1_0\n",
    "print(f\"Camera 1 to World Rotation:\\n{R_1_w}\")\n",
    "print(f\"Camera 1 to World Translation:\\n{T_1_w}\")\n",
    "\n",
    "# save all calibration data\n",
    "camera_calib_data[\"cam0\"][\"extrinsics\"][\"R\"] = R_0_w\n",
    "camera_calib_data[\"cam0\"][\"extrinsics\"][\"T\"] = T_0_w\n",
    "camera_calib_data[\"cam1\"][\"extrinsics\"][\"R\"] = R_1_w\n",
    "camera_calib_data[\"cam1\"][\"extrinsics\"][\"T\"] = T_1_w\n",
    "\n",
    "# with open(f\"{calib_base_folder}/camera_calib_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(camera_calib_data, f)\n",
    "\n",
    "cv2.imshow(\"Camera 0 - Aruco Marker with World Frame\", img)\n",
    "# cv2.imwrite(f\"{calib_base_folder}/extrinsic_calib_images/cam0/img_aruco_with_axes.png\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera 0 to World Rotation:\n",
      " [[ 0.96241617  0.07780901  0.26019391]\n",
      " [ 0.002285    0.95572452 -0.29425401]\n",
      " [-0.27156931  0.28378936  0.91962694]]\n",
      "Camera 0 to World Translation:\n",
      " [[-0.00391395]\n",
      " [-0.03319903]\n",
      " [ 0.20169472]]\n",
      "Camera 1 to World Rotation:\n",
      " [[ 0.27576602 -0.25516868 -0.92672725]\n",
      " [-0.05900692  0.95778878 -0.28126257]\n",
      " [ 0.95939296  0.13223629  0.24905744]]\n",
      "Camera 1 to World Translation:\n",
      " [[-0.03173681]\n",
      " [-0.01782421]\n",
      " [ 0.22452266]]\n"
     ]
    }
   ],
   "source": [
    "# Verify camera to world transform by projecting axes onto image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import cv2.aruco as aruco\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load calibration data\n",
    "with open(f\"../camera_calibration/05-08-25/camera_calib_data.pkl\", \"rb\") as f:\n",
    "    camera_calib_data = pickle.load(f)\n",
    "\n",
    "# Load intrinsic and extrinsic parameters for both cameras\n",
    "K0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "R_0_w = camera_calib_data[\"cam0\"][\"extrinsics\"][\"R\"]\n",
    "T_0_w = camera_calib_data[\"cam0\"][\"extrinsics\"][\"T\"]\n",
    "print(\"Camera 0 to World Rotation:\\n\", R_0_w)\n",
    "print(\"Camera 0 to World Translation:\\n\", T_0_w)\n",
    "\n",
    "K1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "dist1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "R_1_w = camera_calib_data[\"cam1\"][\"extrinsics\"][\"R\"]\n",
    "T_1_w = camera_calib_data[\"cam1\"][\"extrinsics\"][\"T\"]\n",
    "print(\"Camera 1 to World Rotation:\\n\", R_1_w)\n",
    "print(\"Camera 1 to World Translation:\\n\", T_1_w)\n",
    "\n",
    "# Load images for verification\n",
    "img0 = cv2.imread(f\"../tip_pose_images/cam0_0.png\")\n",
    "img1 = cv2.imread(f\"../tip_pose_images/cam1_0.png\")\n",
    "\n",
    "# Draw world frame axes on Camera 0 image\n",
    "draw_axes(img0, R_0_w, T_0_w, K0, dist0)\n",
    "cv2.imshow(\"Camera 0 - World Frame Projection\", img0)\n",
    "\n",
    "# Draw world frame axes on Camera 1 image\n",
    "draw_axes(img1, R_1_w, T_1_w, K1, dist1)\n",
    "cv2.imshow(\"Camera 1 - World Frame Projection\", img1)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1:\n",
      " [[-0.00941198  0.99909794 -0.04118328]\n",
      " [-0.0493389   0.04065417  0.99792921]\n",
      " [ 0.99871333  0.01144134  0.04890463]]\n",
      "T1:\n",
      " [[-0.01961234]\n",
      " [-0.02486894]\n",
      " [ 0.2308288 ]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"../camera_calibration/05-08-25/camera_calib_data.pkl\", \"rb\") as f:\n",
    "    camera_calib = pickle.load(f)\n",
    "\n",
    "K0 = camera_calib[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist0 = camera_calib[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "R_1_0 = camera_calib[\"stereo_extrinsics\"][\"R_1_0\"]\n",
    "T_1_0 = camera_calib[\"stereo_extrinsics\"][\"T_1_0\"]\n",
    "\n",
    "img_points = np.array([\n",
    "    [317., 117.],\n",
    "    [318., 144.],\n",
    "    [316., 199.],\n",
    "    [308., 118.],\n",
    "    # [278, 148],\n",
    "    # [326, 118],\n",
    "    # [354, 150]\n",
    "], dtype=np.float32)\n",
    "obj_points = 0.001 * np.array([[0, 4.125, 0],\n",
    "                [0, 16, 9],\n",
    "                [0, 0, 23.95],\n",
    "                [-4.125, 0, 0],\n",
    "                # [-16, 0, 9],\n",
    "                # [4.125, 0, 0],\n",
    "                # [16, 0, 9]\n",
    "                ])\n",
    "\n",
    "ret, rvec, tvec, _ = cv2.solvePnPRansac(obj_points, img_points, K0, dist0, flags=cv2.SOLVEPNP_P3P)\n",
    "R0 = cv2.Rodrigues(rvec)[0]\n",
    "T0 = tvec\n",
    "R1 = np.dot(R_1_0, R0)\n",
    "T1 = np.dot(R_1_0, T0) + T_1_0\n",
    "print(\"R1:\\n\", R1)\n",
    "print(\"T1:\\n\", T1)\n",
    "\n",
    "# Update and save camera calibration data\n",
    "camera_calib[\"cam0\"][\"extrinsics\"][\"R\"] = R0\n",
    "camera_calib[\"cam0\"][\"extrinsics\"][\"T\"] = T0\n",
    "camera_calib[\"cam1\"][\"extrinsics\"][\"R\"] = R1\n",
    "camera_calib[\"cam1\"][\"extrinsics\"][\"T\"] = T1\n",
    "\n",
    "with open(f\"../camera_calibration/05-16-25/camera_calib_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(camera_calib, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize new transform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def draw_axes(image, R, T, K, dist):\n",
    "    \"\"\"Draw the world frame origin and axes on the image.\"\"\"\n",
    "    axis_length = 0.05  # 5 cm\n",
    "\n",
    "    # Define the 3D points for the axes\n",
    "    axis_points = np.float32([\n",
    "        [0, 0, 0],              # Origin\n",
    "        [axis_length, 0, 0],    # X-axis (red)\n",
    "        [0, axis_length, 0],    # Y-axis (green)\n",
    "        [0, 0, axis_length]     # Z-axis (blue)\n",
    "    ]).reshape(-1, 3)\n",
    "\n",
    "    # Project the 3D axis points onto the image plane\n",
    "    img_points, _ = cv2.projectPoints(axis_points, cv2.Rodrigues(R)[0], T, K, dist)\n",
    "\n",
    "    # Draw the origin point\n",
    "    origin = tuple(img_points[0].ravel().astype(int))\n",
    "    cv2.circle(image, origin, 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Draw the X, Y, Z axes\n",
    "    cv2.line(image, origin, tuple(img_points[1].ravel().astype(int)), (0, 0, 255), 3)  # X-axis (red)\n",
    "    cv2.line(image, origin, tuple(img_points[2].ravel().astype(int)), (0, 255, 0), 3)  # Y-axis (green)\n",
    "    cv2.line(image, origin, tuple(img_points[3].ravel().astype(int)), (255, 0, 0), 3)  # Z-axis (blue)\n",
    "\n",
    "    # Annotate the axes\n",
    "    cv2.putText(image, \"X\", tuple(img_points[1].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(image, \"Y\", tuple(img_points[2].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(image, \"Z\", tuple(img_points[3].ravel().astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "with open(f\"../camera_calibration/05-16-25/camera_calib_data.pkl\", \"rb\") as f:\n",
    "    camera_calib = pickle.load(f)\n",
    "\n",
    "K0 = camera_calib[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist0 = camera_calib[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "R0 = camera_calib[\"cam0\"][\"extrinsics\"][\"R\"]\n",
    "T0 = camera_calib[\"cam0\"][\"extrinsics\"][\"T\"]\n",
    "K1 = camera_calib[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "dist1 = camera_calib[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "R1 = camera_calib[\"cam1\"][\"extrinsics\"][\"R\"]\n",
    "T1 = camera_calib[\"cam1\"][\"extrinsics\"][\"T\"]\n",
    "\n",
    "# Load images for verification\n",
    "img0 = cv2.imread(\"../camera_calibration/05-16-25/extrinsic_calib_images/cam0_0.png\")\n",
    "img1 = cv2.imread(\"../camera_calibration/05-16-25/extrinsic_calib_images/cam1_0.png\")\n",
    "# Downsize images for display\n",
    "img0 = cv2.resize(img0, (640, 480))\n",
    "img1 = cv2.resize(img1, (640, 480))\n",
    "\n",
    "# Draw world frame axes on Camera 0 image\n",
    "draw_axes(img0, R0, T0, K0, dist0)\n",
    "cv2.imshow(\"Camera 0 - World Frame Projection\", img0)\n",
    "# Draw world frame axes on Camera 1 image\n",
    "draw_axes(img1, R1, T1, K1, dist1)\n",
    "cv2.imshow(\"Camera 1 - World Frame Projection\", img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write calibration matrices to python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load camera calibration data from pkl file\n",
    "with open(\"../camera_calibration/05-16-25/camera_calib_data.pkl\", \"rb\") as f:\n",
    "    camera_calib_data = pickle.load(f)\n",
    "\n",
    "# Write to a python module\n",
    "calib_module_path = \"camera_calib_data.py\"\n",
    "with open(calib_module_path, \"w\") as f:\n",
    "    f.write(\"# This file is auto-generated from camera calibration data.\\n\")\n",
    "    f.write(\"import numpy as np\\n\\n\")\n",
    "    f.write(f\"K0 = np.{repr(camera_calib_data['cam0']['intrinsics']['K'])}\\n\")\n",
    "    f.write(f\"d0 = np.{repr(camera_calib_data['cam0']['intrinsics']['d'])}\\n\")\n",
    "    f.write(f\"K1 = np.{repr(camera_calib_data['cam1']['intrinsics']['K'])}\\n\")\n",
    "    f.write(f\"d1 = np.{repr(camera_calib_data['cam1']['intrinsics']['d'])}\\n\")\n",
    "    f.write(f\"R_1_0 = np.{repr(camera_calib_data['stereo_extrinsics']['R_1_0'])}\\n\")\n",
    "    f.write(f\"T_1_0 = np.{repr(camera_calib_data['stereo_extrinsics']['T_1_0'])}\\n\")\n",
    "    f.write(f\"R_0_w = np.{repr(camera_calib_data['cam0']['extrinsics']['R'])}\\n\")\n",
    "    f.write(f\"T_0_w = np.{repr(camera_calib_data['cam0']['extrinsics']['T'])}\\n\")\n",
    "    f.write(f\"R_1_w = np.{repr(camera_calib_data['cam1']['extrinsics']['R'])}\\n\")\n",
    "    f.write(f\"T_1_w = np.{repr(camera_calib_data['cam1']['extrinsics']['T'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[629.50286944   0.         307.21941529]\n",
      " [  0.         630.77155779 250.59295032]\n",
      " [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from camera_calib_data import *\n",
    "\n",
    "print(K0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
