{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Camera 0 refers to top camera, Camera 1 refers to bottom camera\n",
    "# Ensure camera port ids are correct\n",
    "port_ids = [2, 0]\n",
    "cam0_device = f\"/dev/video{port_ids[0]}\"\n",
    "cam1_device = f\"/dev/video{port_ids[1]}\"\n",
    "\n",
    "# Ensure proper camera configurations\n",
    "cam0_focus_value = 35\n",
    "cam1_focus_value = 75\n",
    "config_commands = {cam0_device: [\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_absolute={cam0_focus_value}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ],\n",
    "                cam1_device: [\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_absolute={cam1_focus_value}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "def configure_camera(devices, config_commands):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        for command in config_commands[device]:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab some test images of catheter tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect images of calibration board in both cameras frames for stereo extrinsic calibration\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "output_dir = f\"../tip_pose_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Make sure cameras are configures\n",
    "# configure_camera([cam0_device, cam1_device], config_commands) # Uncomment to use default configs\n",
    "cap0 = cv2.VideoCapture(port_ids[0], cv2.CAP_V4L2)\n",
    "cap1 = cv2.VideoCapture(port_ids[1], cv2.CAP_V4L2)\n",
    "frame_count = 0\n",
    "while True:\n",
    "    # Read frames from both cameras\n",
    "    ret0, frame0 = cap0.read()\n",
    "    ret1, frame1 = cap1.read()\n",
    "\n",
    "    if not ret0 or not ret1:\n",
    "        print(\"Error: One or both frames could not be read.\")\n",
    "        break\n",
    "\n",
    "    # Display both camera feeds with timestamps\n",
    "    # timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")\n",
    "    # cv2.putText(frame1, f\"Cam1 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    # cv2.putText(frame2, f\"Cam2 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine and display both frames\n",
    "    combined = cv2.hconcat([frame0, frame1])\n",
    "    cv2.imshow(\"Camera 0 (top) + Camera 1 (side)\", combined)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord(' '):  # Space key to capture images\n",
    "        img0_path = f\"{output_dir}/cam0_{frame_count}.png\"\n",
    "        img1_path = f\"{output_dir}/cam1_{frame_count}.png\"\n",
    "        cv2.imwrite(img0_path, frame0)\n",
    "        cv2.imwrite(img1_path, frame1)\n",
    "        print(f\"Captured images:\\n - {img0_path}\\n - {img1_path}\")\n",
    "        frame_count += 1\n",
    "\n",
    "# Release both cameras and close windows\n",
    "cap0.release()\n",
    "cap1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation by manual point prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Mouse callback function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global clicked_points, mode\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        label = 1 if mode == 'f' else 0\n",
    "        clicked_points.append((x, y, label))\n",
    "        color = (0, 255, 0) if label == 1 else (0, 0, 255)\n",
    "        cv2.circle(image, (x, y), 5, color, -1)\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        print(f\"Clicked point: x={x}, y={y}, label={label}\")\n",
    "\n",
    "# Load SAM model\n",
    "checkpoint_path = \"/home/arclab/repos/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "# sam.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam.to(\"cpu\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# Loop over images in director\n",
    "dir = \"../tip_pose_images\"\n",
    "cam0_img_path = sorted(glob.glob(f\"{dir}/cam0_*.png\"))\n",
    "cam1_img_path = sorted(glob.glob(f\"{dir}/cam1_*.png\"))\n",
    "mask_dir = f\"{dir}/masks\"\n",
    "\n",
    "for cam_num, (img0_path, img1_path) in enumerate(zip(cam0_img_path, cam1_img_path)):\n",
    "    # Read images\n",
    "    img0 = cv2.imread(img0_path)\n",
    "    img1 = cv2.imread(img1_path)\n",
    "\n",
    "    # Convert to RGB\n",
    "    img0_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get image dimensions\n",
    "    h, w, _ = img0.shape\n",
    "\n",
    "    for image_num, image in enumerate([img0, img1]):\n",
    "\n",
    "        sam_predictor.set_image(image)\n",
    "\n",
    "       # Initialize global variables\n",
    "        clicked_points = []\n",
    "        mode = 'f'  # Start with foreground mode\n",
    "\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        cv2.setMouseCallback(\"Select Points (f: foreground, b: background, ESC: done)\", on_mouse)\n",
    "\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 27:  # ESC to exit\n",
    "                break\n",
    "            elif key == ord('f'):\n",
    "                mode = 'f'\n",
    "                print(\"Switched to foreground mode.\")\n",
    "            elif key == ord('b'):\n",
    "                mode = 'b'\n",
    "                print(\"Switched to background mode.\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Separate points into foreground and background\n",
    "        fg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 1], dtype=np.float32)\n",
    "        bg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 0], dtype=np.float32)\n",
    "\n",
    "        # Predict the mask\n",
    "        with torch.no_grad():\n",
    "            masks, scores, logits = sam_predictor.predict(\n",
    "                point_coords=np.vstack((fg_coords, bg_coords)),\n",
    "                point_labels=np.array([1] * len(fg_coords) + [0] * len(bg_coords)),\n",
    "                multimask_output=False\n",
    "            )\n",
    "\n",
    "        # Save segmentation results\n",
    "        segmentation_results = {\n",
    "            \"masks\": masks,\n",
    "            \"scores\": scores,\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        with open(f\"{dir}/segmentation_results/cam{cam_num}_{image_num}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(segmentation_results, f)\n",
    "\n",
    "        # Display and save the mask overlayed on the original image\n",
    "        mask = masks[0].astype(np.uint8) * 255\n",
    "        # Create a red mask overlay with transparency\n",
    "        mask_colored = np.zeros_like(image)\n",
    "        mask_colored[:, :, 2] = mask  # Red channel\n",
    "        overlay = image.copy()\n",
    "        alpha = 0.5\n",
    "        overlay[mask > 0] = (1 - alpha) * overlay[mask > 0] + alpha * np.array([0, 0, 255])\n",
    "        cv2.imshow(\"Segmented Mask Overlay\", overlay)\n",
    "        cv2.imwrite(f\"{mask_dir}/cam{cam_num}_{image_num}.png\", overlay)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape from Silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create voxel projection lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0145 -0.0145  0.    ]\n",
      " [-0.0145 -0.0145  0.001 ]\n",
      " [-0.0145 -0.0145  0.002 ]\n",
      " ...\n",
      " [ 0.0145  0.0145  0.027 ]\n",
      " [ 0.0145  0.0145  0.028 ]\n",
      " [ 0.0145  0.0145  0.029 ]]\n",
      "Min image coordinates: [[265.65885151 111.25336612]]\n",
      "Max image coordinates: [[370.22327543 216.82091068]]\n",
      "Min image coordinates: [[227.70646288 179.00610686]]\n",
      "Max image coordinates: [[315.77846036 266.47969097]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "# Define voxel grid\n",
    "VOXEL_SIZE = 0.001  # 1 mm voxel size\n",
    "N_X = 30\n",
    "N_Y = 30\n",
    "N_Z = 30\n",
    "voxel_map = np.zeros((N_X, N_Y, N_Z), dtype=np.uint8)\n",
    "origin = np.array([(N_X - 1) / 2, (N_Y - 1) / 2, 0]) * VOXEL_SIZE\n",
    "# origin = np.array([0, 0, 0])  # Center of the voxel grid\n",
    "voxel_coordinates = np.mgrid[0:N_X, 0:N_Y, 0:N_Z].reshape(3, -1).T * VOXEL_SIZE - origin\n",
    "print(voxel_coordinates)\n",
    "\n",
    "# def voxel_index_to_cartesian(i, j, k):\n",
    "#     '''Convert voxel indices to 3D coordinates in the voxel grid.\n",
    "#     Coordinate frame origin is at center of x, y, and at 0 for z.\n",
    "#     '''\n",
    "#     x_coord = (i - (N_X - 1) / 2) * VOXEL_SIZE\n",
    "#     y_coord = (j - (N_Y - 1) / 2) * VOXEL_SIZE\n",
    "#     z_coord = k * VOXEL_SIZE\n",
    "#     return np.array([x_coord, y_coord, z_coord])\n",
    "\n",
    "# Load camera calibration data\n",
    "with open(\"../camera_calibration/05-16-25/camera_calib_data.pkl\", \"rb\") as f:\n",
    "    camera_calib_data = pickle.load(f)\n",
    "\n",
    "# Define lookup table for two cameras voxel projection to image plane\n",
    "voxel_lookup_table = np.zeros((2, N_X*N_Y*N_Z, 2), dtype=np.float32)\n",
    "\n",
    "for cam_num in range(2):\n",
    "        # Get camera parameters\n",
    "        K = camera_calib_data[f\"cam{cam_num}\"][\"intrinsics\"][\"K\"] # Camera intrinsic matrix\n",
    "        d = camera_calib_data[f\"cam{cam_num}\"][\"intrinsics\"][\"d\"] # Distortion coefficients\n",
    "        R = camera_calib_data[f\"cam{cam_num}\"][\"extrinsics\"][\"R\"] # Extrinsic camera-world rotation matrix\n",
    "        T = camera_calib_data[f\"cam{cam_num}\"][\"extrinsics\"][\"T\"] # Extrinsic camera-world translation vector\n",
    "        rvec, _ = cv2.Rodrigues(R) # Rotation vector from rotation matrix\n",
    "        image_coordinates, _ = cv2.projectPoints(voxel_coordinates, rvec, T, K, d)\n",
    "        print(f\"Min image coordinates: {np.min(image_coordinates, axis=0)}\")\n",
    "        print(f\"Max image coordinates: {np.max(image_coordinates, axis=0)}\")\n",
    "        voxel_lookup_table[cam_num, :, :] = image_coordinates.reshape(-1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Calibration Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "\n",
    "img_dir = \"C:\\\\Users\\\\jlim\\\\OneDrive - Cor Medical Ventures\\\\Documents\\\\Channel Robotics\\\\Catheter Calibration Data\\\\LC_v3_05_16_25_T1\\\\image_snapshots\"\n",
    "img0_paths = glob.glob(f\"{img_dir}\\\\cam_0\\\\*.png\")\n",
    "img1_paths = glob.glob(f\"{img_dir}\\\\cam_1\\\\*.png\")\n",
    "img0 = cv2.imread(img0_paths[0])\n",
    "img1 = cv2.imread(img1_paths[0])\n",
    "\n",
    "# Draw world frame axes projected onto each image\n",
    "\n",
    "# Define world frame origin and axes in 3D (in meters)\n",
    "origin_3d = np.array([[0, 0, 0]], dtype=np.float32)\n",
    "x_axis_3d = np.array([[0.05, 0, 0]], dtype=np.float32)  # 1cm along X\n",
    "y_axis_3d = np.array([[0, 0.05, 0]], dtype=np.float32)  # 1cm along Y\n",
    "z_axis_3d = np.array([[0, 0, 0.05]], dtype=np.float32)  # 1cm along Z\n",
    "\n",
    "# Project to image 0\n",
    "K0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "d0 = camera_calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "R0 = camera_calib_data[\"cam0\"][\"extrinsics\"][\"R\"]\n",
    "T0 = camera_calib_data[\"cam0\"][\"extrinsics\"][\"T\"]\n",
    "rvec0, _ = cv2.Rodrigues(R0)\n",
    "tvec0 = T0\n",
    "\n",
    "origin_img0, _ = cv2.projectPoints(origin_3d, rvec0, tvec0, K0, d0)\n",
    "x_img0, _ = cv2.projectPoints(x_axis_3d, rvec0, tvec0, K0, d0)\n",
    "y_img0, _ = cv2.projectPoints(y_axis_3d, rvec0, tvec0, K0, d0)\n",
    "z_img0, _ = cv2.projectPoints(z_axis_3d, rvec0, tvec0, K0, d0)\n",
    "\n",
    "# Draw axes on img0\n",
    "pt0 = tuple(origin_img0[0, 0].astype(int))\n",
    "ptx = tuple(x_img0[0, 0].astype(int))\n",
    "pty = tuple(y_img0[0, 0].astype(int))\n",
    "ptz = tuple(z_img0[0, 0].astype(int))\n",
    "cv2.arrowedLine(img0, pt0, ptx, (0, 0, 255), 2, tipLength=0.3)  # X - Red\n",
    "cv2.arrowedLine(img0, pt0, pty, (0, 255, 0), 2, tipLength=0.3)  # Y - Green\n",
    "cv2.arrowedLine(img0, pt0, ptz, (255, 0, 0), 2, tipLength=0.3)  # Z - Blue\n",
    "\n",
    "# Project to image 1\n",
    "K1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "d1 = camera_calib_data[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "R1 = camera_calib_data[\"cam1\"][\"extrinsics\"][\"R\"]\n",
    "T1 = camera_calib_data[\"cam1\"][\"extrinsics\"][\"T\"]\n",
    "rvec1, _ = cv2.Rodrigues(R1)\n",
    "tvec1 = T1\n",
    "\n",
    "origin_img1, _ = cv2.projectPoints(origin_3d, rvec1, tvec1, K1, d1)\n",
    "x_img1, _ = cv2.projectPoints(x_axis_3d, rvec1, tvec1, K1, d1)\n",
    "y_img1, _ = cv2.projectPoints(y_axis_3d, rvec1, tvec1, K1, d1)\n",
    "z_img1, _ = cv2.projectPoints(z_axis_3d, rvec1, tvec1, K1, d1)\n",
    "\n",
    "# Draw axes on img1\n",
    "pt0_1 = tuple(origin_img1[0, 0].astype(int))\n",
    "ptx_1 = tuple(x_img1[0, 0].astype(int))\n",
    "pty_1 = tuple(y_img1[0, 0].astype(int))\n",
    "ptz_1 = tuple(z_img1[0, 0].astype(int))\n",
    "cv2.arrowedLine(img1, pt0_1, ptx_1, (0, 0, 255), 2, tipLength=0.3)  # X - Red\n",
    "cv2.arrowedLine(img1, pt0_1, pty_1, (0, 255, 0), 2, tipLength=0.3)  # Y - Green\n",
    "cv2.arrowedLine(img1, pt0_1, ptz_1, (255, 0, 0), 2, tipLength=0.3)  # Z - Blue\n",
    "\n",
    "cv2.imshow(\"Image 0\", img0)\n",
    "cv2.imshow(\"Image 1\", img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment images\n",
    "\n",
    "# Mouse callback function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global clicked_points, mode\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        label = 1 if mode == 'f' else 0\n",
    "        clicked_points.append((x, y, label))\n",
    "        color = (0, 255, 0) if label == 1 else (0, 0, 255)\n",
    "        cv2.circle(image, (x, y), 5, color, -1)\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        print(f\"Clicked point: x={x}, y={y}, label={label}\")\n",
    "\n",
    "# Load SAM model\n",
    "checkpoint_path = \"/home/arclab/repos/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "# sam.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam.to(\"cpu\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# Loop over images in director\n",
    "dir = \"../tip_pose_images\"\n",
    "cam0_img_path = sorted(glob.glob(f\"{dir}/cam0_*.png\"))\n",
    "cam1_img_path = sorted(glob.glob(f\"{dir}/cam1_*.png\"))\n",
    "mask_dir = f\"{dir}/masks\"\n",
    "\n",
    "for cam_num, (img0_path, img1_path) in enumerate(zip(cam0_img_path, cam1_img_path)):\n",
    "    # Read images\n",
    "    img0 = cv2.imread(img0_path)\n",
    "    img1 = cv2.imread(img1_path)\n",
    "\n",
    "    # Convert to RGB\n",
    "    img0_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get image dimensions\n",
    "    h, w, _ = img0.shape\n",
    "\n",
    "    for image_num, image in enumerate([img0, img1]):\n",
    "\n",
    "        sam_predictor.set_image(image)\n",
    "\n",
    "       # Initialize global variables\n",
    "        clicked_points = []\n",
    "        mode = 'f'  # Start with foreground mode\n",
    "\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        cv2.setMouseCallback(\"Select Points (f: foreground, b: background, ESC: done)\", on_mouse)\n",
    "\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 27:  # ESC to exit\n",
    "                break\n",
    "            elif key == ord('f'):\n",
    "                mode = 'f'\n",
    "                print(\"Switched to foreground mode.\")\n",
    "            elif key == ord('b'):\n",
    "                mode = 'b'\n",
    "                print(\"Switched to background mode.\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Separate points into foreground and background\n",
    "        fg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 1], dtype=np.float32)\n",
    "        bg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 0], dtype=np.float32)\n",
    "\n",
    "        # Predict the mask\n",
    "        with torch.no_grad():\n",
    "            masks, scores, logits = sam_predictor.predict(\n",
    "                point_coords=np.vstack((fg_coords, bg_coords)),\n",
    "                point_labels=np.array([1] * len(fg_coords) + [0] * len(bg_coords)),\n",
    "                multimask_output=False\n",
    "            )\n",
    "\n",
    "        # Save segmentation results\n",
    "        segmentation_results = {\n",
    "            \"masks\": masks,\n",
    "            \"scores\": scores,\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        with open(f\"{dir}/segmentation_results/cam{cam_num}_{image_num}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(segmentation_results, f)\n",
    "\n",
    "        # Display and save the mask overlayed on the original image\n",
    "        mask = masks[0].astype(np.uint8) * 255\n",
    "        # Create a red mask overlay with transparency\n",
    "        mask_colored = np.zeros_like(image)\n",
    "        mask_colored[:, :, 2] = mask  # Red channel\n",
    "        overlay = image.copy()\n",
    "        alpha = 0.5\n",
    "        overlay[mask > 0] = (1 - alpha) * overlay[mask > 0] + alpha * np.array([0, 0, 255])\n",
    "        cv2.imshow(\"Segmented Mask Overlay\", overlay)\n",
    "        cv2.imwrite(f\"{mask_dir}/cam{cam_num}_{image_num}.png\", overlay)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel Carving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -795 is out of bounds for axis 1 with size 640",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     cam1_voxel_coords \u001b[38;5;241m=\u001b[39m voxel_lookup_table[\u001b[38;5;241m1\u001b[39m, index, :]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Check if the voxel is visible in both cameras\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cam0_mask[\u001b[38;5;28mint\u001b[39m(cam0_voxel_coords[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mint\u001b[39m(cam0_voxel_coords[\u001b[38;5;241m0\u001b[39m])] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m---> 31\u001b[0m         cam1_mask[\u001b[38;5;28mint\u001b[39m(cam1_voxel_coords[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mint\u001b[39m(cam1_voxel_coords[\u001b[38;5;241m0\u001b[39m])] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m         voxel_map[i, j, k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Visualize voxel map\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Extract the coordinates of occupied voxels\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -795 is out of bounds for axis 1 with size 640"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir = \"../tip_pose_images\"\n",
    "# cam0_path = sorted(glob.glob(f\"{dir}/cam0_*.pkl\"))\n",
    "# cam1_path = sorted(glob.glob(f\"{dir}/cam1_*.pkl\"))\n",
    "cam0_file = f\"{dir}/segmentation_results/cam0_0.pkl\"\n",
    "cam1_file = f\"{dir}/segmentation_results/cam1_0.pkl\"\n",
    "\n",
    "# for cam0_file, cam1_file in zip(cam0_path, cam1_path):\n",
    "\n",
    "with open(cam0_file, \"rb\") as f:\n",
    "    cam0_segmentation_results = pickle.load(f)\n",
    "with open(cam1_file, \"rb\") as f:\n",
    "    cam1_segmentation_results = pickle.load(f)\n",
    "cam0_mask = cam0_segmentation_results[\"masks\"][0]\n",
    "cam1_mask = cam1_segmentation_results[\"masks\"][0]\n",
    "\n",
    "# Voxel carving\n",
    "for index, (i, j, k) in enumerate(np.ndindex(N_X, N_Y, N_Z)):\n",
    "    cam0_voxel_coords = voxel_lookup_table[0, index, :]\n",
    "    cam1_voxel_coords = voxel_lookup_table[1, index, :]\n",
    "\n",
    "    # Check if the voxel is visible in both cameras\n",
    "    if cam0_mask[int(cam0_voxel_coords[1]), int(cam0_voxel_coords[0])] > 0 and \\\n",
    "        cam1_mask[int(cam1_voxel_coords[1]), int(cam1_voxel_coords[0])] > 0:\n",
    "        voxel_map[i, j, k] = 1\n",
    "\n",
    "\n",
    "# Visualize voxel map\n",
    "\n",
    "# Extract the coordinates of occupied voxels\n",
    "occupied_voxels = np.argwhere(voxel_map == 1)\n",
    "occupied_points = occupied_voxels * VOXEL_SIZE - origin\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(occupied_points[:, 0], occupied_points[:, 1], occupied_points[:, 2], c='r', marker='o')\n",
    "\n",
    "# Set labels and aspect ratio\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_box_aspect([1, 1, 1])  # Equal aspect ratio\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
