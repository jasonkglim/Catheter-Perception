{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Camera 0 refers to top camera, Camera 1 refers to bottom camera\n",
    "# Ensure camera port ids are correct\n",
    "port_ids = [2, 0]\n",
    "cam0_device = f\"/dev/video{port_ids[0]}\"\n",
    "cam1_device = f\"/dev/video{port_ids[1]}\"\n",
    "\n",
    "# Ensure proper camera configurations\n",
    "cam0_focus_value = 35\n",
    "cam1_focus_value = 75\n",
    "config_commands = {cam0_device: [\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_absolute={cam0_focus_value}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ],\n",
    "                cam1_device: [\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_absolute={cam1_focus_value}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "def configure_camera(devices, config_commands):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        for command in config_commands[device]:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab some test images of catheter tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured images:\n",
      " - ../tip_pose_images/cam0_0.png\n",
      " - ../tip_pose_images/cam1_0.png\n",
      "Captured images:\n",
      " - ../tip_pose_images/cam0_1.png\n",
      " - ../tip_pose_images/cam1_1.png\n",
      "Captured images:\n",
      " - ../tip_pose_images/cam0_2.png\n",
      " - ../tip_pose_images/cam1_2.png\n",
      "Captured images:\n",
      " - ../tip_pose_images/cam0_3.png\n",
      " - ../tip_pose_images/cam1_3.png\n"
     ]
    }
   ],
   "source": [
    "# Collect images of calibration board in both cameras frames for stereo extrinsic calibration\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "output_dir = f\"../tip_pose_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Make sure cameras are configures\n",
    "# configure_camera([cam0_device, cam1_device], config_commands) # Uncomment to use default configs\n",
    "cap0 = cv2.VideoCapture(port_ids[0], cv2.CAP_V4L2)\n",
    "cap1 = cv2.VideoCapture(port_ids[1], cv2.CAP_V4L2)\n",
    "frame_count = 0\n",
    "while True:\n",
    "    # Read frames from both cameras\n",
    "    ret0, frame0 = cap0.read()\n",
    "    ret1, frame1 = cap1.read()\n",
    "\n",
    "    if not ret0 or not ret1:\n",
    "        print(\"Error: One or both frames could not be read.\")\n",
    "        break\n",
    "\n",
    "    # Display both camera feeds with timestamps\n",
    "    # timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")\n",
    "    # cv2.putText(frame1, f\"Cam1 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    # cv2.putText(frame2, f\"Cam2 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine and display both frames\n",
    "    combined = cv2.hconcat([frame0, frame1])\n",
    "    cv2.imshow(\"Camera 0 (top) + Camera 1 (side)\", combined)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord(' '):  # Space key to capture images\n",
    "        img0_path = f\"{output_dir}/cam0_{frame_count}.png\"\n",
    "        img1_path = f\"{output_dir}/cam1_{frame_count}.png\"\n",
    "        cv2.imwrite(img0_path, frame0)\n",
    "        cv2.imwrite(img1_path, frame1)\n",
    "        print(f\"Captured images:\\n - {img0_path}\\n - {img1_path}\")\n",
    "        frame_count += 1\n",
    "\n",
    "# Release both cameras and close windows\n",
    "cap0.release()\n",
    "cap1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation by manual point prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arclab/repos/segment-anything/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to foreground mode.\n",
      "Clicked point: x=313, y=170, label=1\n",
      "Clicked point: x=311, y=200, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=314, y=92, label=0\n",
      "Clicked point: x=436, y=196, label=0\n",
      "Clicked point: x=289, y=219, label=1\n",
      "Clicked point: x=288, y=253, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=288, y=158, label=0\n",
      "Clicked point: x=345, y=219, label=0\n",
      "Clicked point: x=323, y=185, label=1\n",
      "Clicked point: x=354, y=227, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=312, y=102, label=0\n",
      "Clicked point: x=367, y=175, label=0\n",
      "Clicked point: x=286, y=243, label=1\n",
      "Switched to background mode.\n",
      "Switched to foreground mode.\n",
      "Clicked point: x=288, y=264, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=285, y=175, label=0\n",
      "Clicked point: x=332, y=254, label=0\n",
      "Clicked point: x=318, y=167, label=1\n",
      "Clicked point: x=356, y=227, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=312, y=96, label=0\n",
      "Clicked point: x=387, y=154, label=0\n",
      "Clicked point: x=287, y=236, label=1\n",
      "Clicked point: x=290, y=264, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=289, y=162, label=0\n",
      "Clicked point: x=370, y=229, label=0\n",
      "Clicked point: x=311, y=176, label=1\n",
      "Clicked point: x=309, y=216, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=313, y=117, label=0\n",
      "Clicked point: x=390, y=159, label=0\n",
      "Clicked point: x=293, y=226, label=1\n",
      "Clicked point: x=327, y=265, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=288, y=176, label=0\n",
      "Clicked point: x=343, y=220, label=0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Mouse callback function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global clicked_points, mode\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        label = 1 if mode == 'f' else 0\n",
    "        clicked_points.append((x, y, label))\n",
    "        color = (0, 255, 0) if label == 1 else (0, 0, 255)\n",
    "        cv2.circle(image, (x, y), 5, color, -1)\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        print(f\"Clicked point: x={x}, y={y}, label={label}\")\n",
    "\n",
    "# Load SAM model\n",
    "checkpoint_path = \"/home/arclab/repos/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "# sam.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam.to(\"cpu\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# Loop over images in director\n",
    "dir = \"../tip_pose_images\"\n",
    "cam0_img_path = sorted(glob.glob(f\"{dir}/cam0_*.png\"))\n",
    "cam1_img_path = sorted(glob.glob(f\"{dir}/cam1_*.png\"))\n",
    "mask_dir = f\"{dir}/masks\"\n",
    "\n",
    "for cam_num, (img0_path, img1_path) in enumerate(zip(cam0_img_path, cam1_img_path)):\n",
    "    # Read images\n",
    "    img0 = cv2.imread(img0_path)\n",
    "    img1 = cv2.imread(img1_path)\n",
    "\n",
    "    # Convert to RGB\n",
    "    img0_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get image dimensions\n",
    "    h, w, _ = img0.shape\n",
    "\n",
    "    for image_num, image in enumerate([img0, img1]):\n",
    "\n",
    "        sam_predictor.set_image(image)\n",
    "\n",
    "       # Initialize global variables\n",
    "        clicked_points = []\n",
    "        mode = 'f'  # Start with foreground mode\n",
    "\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        cv2.setMouseCallback(\"Select Points (f: foreground, b: background, ESC: done)\", on_mouse)\n",
    "\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 27:  # ESC to exit\n",
    "                break\n",
    "            elif key == ord('f'):\n",
    "                mode = 'f'\n",
    "                print(\"Switched to foreground mode.\")\n",
    "            elif key == ord('b'):\n",
    "                mode = 'b'\n",
    "                print(\"Switched to background mode.\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Separate points into foreground and background\n",
    "        fg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 1], dtype=np.float32)\n",
    "        bg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 0], dtype=np.float32)\n",
    "\n",
    "        # Predict the mask\n",
    "        with torch.no_grad():\n",
    "            masks, scores, logits = sam_predictor.predict(\n",
    "                point_coords=np.vstack((fg_coords, bg_coords)),\n",
    "                point_labels=np.array([1] * len(fg_coords) + [0] * len(bg_coords)),\n",
    "                multimask_output=False\n",
    "            )\n",
    "\n",
    "        # Display and save the mask\n",
    "        mask = masks[0].astype(np.uint8) * 255\n",
    "        cv2.imshow(\"Segmented Mask\", image)\n",
    "        cv2.imshow(masks[0], alpha=0.5, cmap='Reds')\n",
    "        cv2.imwrite(f\"{mask_dir}/cam{cam_num}_{image_num}.png\", mask)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
