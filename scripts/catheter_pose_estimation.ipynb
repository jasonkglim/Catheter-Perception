{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Camera 0 refers to top camera, Camera 1 refers to bottom camera\n",
    "# Ensure camera port ids are correct\n",
    "port_ids = [2, 0]\n",
    "cam0_device = f\"/dev/video{port_ids[0]}\"\n",
    "cam1_device = f\"/dev/video{port_ids[1]}\"\n",
    "\n",
    "# Ensure proper camera configurations\n",
    "cam0_focus_value = 35\n",
    "cam1_focus_value = 75\n",
    "config_commands = {cam0_device: [\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam0_device} -c focus_absolute={cam0_focus_value}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ],\n",
    "                cam1_device: [\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_automatic_continuous=0\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c auto_exposure=3\",\n",
    "                    f\"v4l2-ctl -d {cam1_device} -c focus_absolute={cam1_focus_value}\",\n",
    "                    # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "                    # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "                    # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "                    # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "                    # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "def configure_camera(devices, config_commands):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        for command in config_commands[device]:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab some test images of catheter tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect images of calibration board in both cameras frames for stereo extrinsic calibration\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "output_dir = f\"../tip_pose_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Make sure cameras are configures\n",
    "# configure_camera([cam0_device, cam1_device], config_commands) # Uncomment to use default configs\n",
    "cap0 = cv2.VideoCapture(port_ids[0], cv2.CAP_V4L2)\n",
    "cap1 = cv2.VideoCapture(port_ids[1], cv2.CAP_V4L2)\n",
    "frame_count = 0\n",
    "while True:\n",
    "    # Read frames from both cameras\n",
    "    ret0, frame0 = cap0.read()\n",
    "    ret1, frame1 = cap1.read()\n",
    "\n",
    "    if not ret0 or not ret1:\n",
    "        print(\"Error: One or both frames could not be read.\")\n",
    "        break\n",
    "\n",
    "    # Display both camera feeds with timestamps\n",
    "    # timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")\n",
    "    # cv2.putText(frame1, f\"Cam1 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    # cv2.putText(frame2, f\"Cam2 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine and display both frames\n",
    "    combined = cv2.hconcat([frame0, frame1])\n",
    "    cv2.imshow(\"Camera 0 (top) + Camera 1 (side)\", combined)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord(' '):  # Space key to capture images\n",
    "        img0_path = f\"{output_dir}/cam0_{frame_count}.png\"\n",
    "        img1_path = f\"{output_dir}/cam1_{frame_count}.png\"\n",
    "        cv2.imwrite(img0_path, frame0)\n",
    "        cv2.imwrite(img1_path, frame1)\n",
    "        print(f\"Captured images:\\n - {img0_path}\\n - {img1_path}\")\n",
    "        frame_count += 1\n",
    "\n",
    "# Release both cameras and close windows\n",
    "cap0.release()\n",
    "cap1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation by manual point prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked point: x=313, y=165, label=1\n",
      "Switched to background mode.\n",
      "Clicked point: x=390, y=207, label=0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Mouse callback function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global clicked_points, mode\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        label = 1 if mode == 'f' else 0\n",
    "        clicked_points.append((x, y, label))\n",
    "        color = (0, 255, 0) if label == 1 else (0, 0, 255)\n",
    "        cv2.circle(image, (x, y), 5, color, -1)\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        print(f\"Clicked point: x={x}, y={y}, label={label}\")\n",
    "\n",
    "# Load SAM model\n",
    "checkpoint_path = \"/home/arclab/repos/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "# sam.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam.to(\"cpu\")\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# Loop over images in director\n",
    "dir = \"../tip_pose_images\"\n",
    "cam0_img_path = sorted(glob.glob(f\"{dir}/cam0_*.png\"))\n",
    "cam1_img_path = sorted(glob.glob(f\"{dir}/cam1_*.png\"))\n",
    "mask_dir = f\"{dir}/masks\"\n",
    "\n",
    "for cam_num, (img0_path, img1_path) in enumerate(zip(cam0_img_path, cam1_img_path)):\n",
    "    # Read images\n",
    "    img0 = cv2.imread(img0_path)\n",
    "    img1 = cv2.imread(img1_path)\n",
    "\n",
    "    # Convert to RGB\n",
    "    img0_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get image dimensions\n",
    "    h, w, _ = img0.shape\n",
    "\n",
    "    for image_num, image in enumerate([img0, img1]):\n",
    "\n",
    "        sam_predictor.set_image(image)\n",
    "\n",
    "       # Initialize global variables\n",
    "        clicked_points = []\n",
    "        mode = 'f'  # Start with foreground mode\n",
    "\n",
    "        cv2.imshow(\"Select Points (f: foreground, b: background, ESC: done)\", image)\n",
    "        cv2.setMouseCallback(\"Select Points (f: foreground, b: background, ESC: done)\", on_mouse)\n",
    "\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 27:  # ESC to exit\n",
    "                break\n",
    "            elif key == ord('f'):\n",
    "                mode = 'f'\n",
    "                print(\"Switched to foreground mode.\")\n",
    "            elif key == ord('b'):\n",
    "                mode = 'b'\n",
    "                print(\"Switched to background mode.\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Separate points into foreground and background\n",
    "        fg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 1], dtype=np.float32)\n",
    "        bg_coords = np.array([[x, y] for x, y, label in clicked_points if label == 0], dtype=np.float32)\n",
    "\n",
    "        # Predict the mask\n",
    "        with torch.no_grad():\n",
    "            masks, scores, logits = sam_predictor.predict(\n",
    "                point_coords=np.vstack((fg_coords, bg_coords)),\n",
    "                point_labels=np.array([1] * len(fg_coords) + [0] * len(bg_coords)),\n",
    "                multimask_output=False\n",
    "            )\n",
    "\n",
    "        # Save segmentation results\n",
    "        segmentation_results = {\n",
    "            \"masks\": masks,\n",
    "            \"scores\": scores,\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        with open(f\"{dir}/segmentation_results/cam{cam_num}_{image_num}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(segmentation_results, f)\n",
    "\n",
    "        # Display and save the mask overlayed on the original image\n",
    "        mask = masks[0].astype(np.uint8) * 255\n",
    "        # Create a red mask overlay with transparency\n",
    "        mask_colored = np.zeros_like(image)\n",
    "        mask_colored[:, :, 2] = mask  # Red channel\n",
    "        overlay = image.copy()\n",
    "        alpha = 0.5\n",
    "        overlay[mask > 0] = (1 - alpha) * overlay[mask > 0] + alpha * np.array([0, 0, 255])\n",
    "        cv2.imshow(\"Segmented Mask Overlay\", overlay)\n",
    "        cv2.imwrite(f\"{mask_dir}/cam{cam_num}_{image_num}.png\", overlay)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape from Silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create voxel projection lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cam0_path = sorted(glob.glob(f\"{dir}/cam0_*.pkl\"))\n",
    "cam1_path = sorted(glob.glob(f\"{dir}/cam1_*.pkl\"))\n",
    "\n",
    "for cam0_file, cam1_file in zip(cam0_path, cam1_path):\n",
    "    with open(cam0_file, \"rb\") as f:\n",
    "        cam0_segmentation_results = pickle.load(f)\n",
    "\n",
    "    with open(cam1_file, \"rb\") as f:\n",
    "        cam1_segmentation_results = pickle.load(f)\n",
    "\n",
    "    \n",
    "    output_file = f\"{dir}/combined_segmentation_results.pkl\"\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(combined_results, f)\n",
    "# Load segmentation results\n",
    "with open(\"../tip_pose_images/segmentation_results/cam0_0.pkl\", \"rb\") as f:\n",
    "    segmentation_results = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
