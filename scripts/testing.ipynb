{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c855375d",
   "metadata": {},
   "source": [
    "# SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "image_bgr = cv2.imread(\"images/frame_20250429_183115.png\")\n",
    "if image_bgr is None:\n",
    "    raise RuntimeError(\"Could not load image.\")\n",
    "\n",
    "image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "image_display = image_bgr.copy()\n",
    "clicked_points = []\n",
    "\n",
    "# Mouse callback function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and len(clicked_points) < 2:\n",
    "        hsv_value = image_hsv[y, x]\n",
    "        clicked_points.append((x, y, hsv_value))\n",
    "\n",
    "        # Draw a small cross\n",
    "        cv2.drawMarker(image_display, (x, y), (0, 0, 255), cv2.MARKER_CROSS, 10, 2)\n",
    "        cv2.imshow(\"Click to sample HSV\", image_display)\n",
    "\n",
    "        print(f\"Clicked point {len(clicked_points)}: x={x}, y={y}, HSV={hsv_value}\")\n",
    "\n",
    "        if len(clicked_points) == 2:\n",
    "            print(\"\\nDone! Use these HSV values to tune your threshold.\")\n",
    "            print(f\"Background HSV: {clicked_points[0][2]}\")\n",
    "            print(f\"Foreground HSV: {clicked_points[1][2]}\")\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "# Display the image and wait for two clicks\n",
    "cv2.imshow(\"Click to sample HSV\", image_display)\n",
    "cv2.setMouseCallback(\"Click to sample HSV\", on_mouse)\n",
    "print(\"Click once on the BACKGROUND, then once on the CATHETER TIP.\")\n",
    "\n",
    "input_point = np.array([clicked_points[0][:2], clicked_points[1][:2]])\n",
    "input_label = np.array([0, 1]) # 0 for background, 1 for foreground\n",
    "print(\"Input point:\", input_point)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50f5ac",
   "metadata": {},
   "source": [
    "# SAM model and image set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2caa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Load SAM ViT-B on CPU\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "checkpoint_path = \"/home/arclab/repos/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "sam.to(\"cuda\")\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Load image and create coarse foreground mask (non-green areas)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "img_bgr = cv2.imread(\"images/frame_20250429_183115.png\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "img_hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "print(\"Original image shape:\", img_bgr.shape)\n",
    "\n",
    "\n",
    "lower_green = np.array([35, 30, 100])\n",
    "upper_green = np.array([85, 255, 255])\n",
    "green_mask = cv2.inRange(img_hsv, lower_green, upper_green)\n",
    "foreground_mask = cv2.bitwise_not(green_mask)\n",
    "binary_mask = np.where(foreground_mask > 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "if img_bgr is None:\n",
    "    raise RuntimeError(\"Could not load image.\")\n",
    "\n",
    "predictor.set_image(img_rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381587a3",
   "metadata": {},
   "source": [
    "# Automated point and box prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d257b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# ============ APPROACH 1: POINT PROMPT =========\n",
    "# ===============================================\n",
    "# Get the centroid of the binary mask\n",
    "M = cv2.moments(binary_mask)\n",
    "if M[\"m00\"] != 0:\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "    point_coords = np.array([[cx, cy]])\n",
    "else:\n",
    "    raise ValueError(\"No foreground pixels found in binary mask\")\n",
    "\n",
    "print(\"Centroid coordinates:\", cx, cy)\n",
    "\n",
    "\n",
    "point_labels = np.array([1])  # 1 = foreground\n",
    "\n",
    "masks_point, scores_point, _ = predictor.predict(\n",
    "    point_coords=point_coords,\n",
    "    point_labels=point_labels,\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# ============ APPROACH 2: BOX PROMPT ===========\n",
    "# ===============================================\n",
    "x, y, w, h = cv2.boundingRect(binary_mask)\n",
    "box = np.array([[x, y, x + w, y + h]])\n",
    "\n",
    "masks_box, scores_box, _ = predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=False\n",
    ")\n",
    "\n",
    "# ---- Visualize Results ----\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axs[0].imshow(image_rgb)\n",
    "axs[0].set_title(\"Original Image\")\n",
    "\n",
    "axs[1].imshow(image_rgb)\n",
    "axs[1].imshow(masks_point[0], alpha=0.5, cmap='Reds')\n",
    "axs[1].scatter(cx, cy, c='blue', s=40)\n",
    "axs[1].set_title(\"Point Prompt Result\")\n",
    "\n",
    "axs[2].imshow(image_rgb)\n",
    "axs[2].imshow(masks_box[0], alpha=0.5, cmap='Reds')\n",
    "axs[2].add_patch(plt.Rectangle((x, y), w, h, edgecolor='blue', facecolor='none', lw=2))\n",
    "axs[2].set_title(\"Box Prompt Result\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_rgb)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define green background HSV range\n",
    "lower_green = np.array([35, 30, 100])\n",
    "upper_green = np.array([85, 255, 255])\n",
    "green_mask = cv2.inRange(img_hsv, lower_green, upper_green)\n",
    "print(\"max val: \", np.max(green_mask))\n",
    "print(\"min val: \", np.min(green_mask))\n",
    "\n",
    "# Invert: 1 for non-green (catheter), 0 for background\n",
    "foreground_mask = cv2.bitwise_not(green_mask)\n",
    "print(\"max val: \", np.max(foreground_mask))\n",
    "\n",
    "# Optionally clean up mask\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_OPEN, kernel)\n",
    "print(\"max val: \", np.max(foreground_mask))\n",
    "# Convert to binary mask\n",
    "binary_mask = np.where(foreground_mask > 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "print(\"min val: \", np.min(binary_mask))\n",
    "print(\"max val: \", np.max(binary_mask))\n",
    "print(\"binary mask shape: \", binary_mask.shape)\n",
    "# Visualize foreground mask\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(binary_mask, cmap='gray')\n",
    "plt.title(\"Foreground Mask (Non-Green Areas)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9345d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Resize the coarse mask to SAM's expected prompt input resolution\n",
    "# # For ViT-B: patch size = 16 → input image (1024) / 16 = 64 → 1/4 = 16x16\n",
    "# prompt_H, prompt_W = 256, 256\n",
    "\n",
    "# coarse_mask = cv2.resize(\n",
    "#     coarse_mask.astype(np.float32),\n",
    "#     (prompt_W, prompt_H),\n",
    "#     interpolation=cv2.INTER_NEAREST\n",
    "# )\n",
    "\n",
    "# print(\"Coarse mask shape:\", coarse_mask.shape)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(coarse_mask)\n",
    "# plt.title(\"Coarse Mask (Non-Green Areas)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # 3. Feed image to SAM\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# image_embedding = predictor.get_image_embedding().cpu().numpy()\n",
    "\n",
    "# print(\"Image embedding shape:\", image_embedding.shape)\n",
    "# print(\"Sample values:\", image_embedding[0, :, :5, :5])  # Print part of it for inspection\n",
    "\n",
    "\n",
    "# # # Add channel dimension → (1, 16, 16)\n",
    "# # mask_input = mask_resized[np.newaxis, :, :]\n",
    "# # print(f\"Mask input shape: {mask_input.shape}\")\n",
    "\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # 4. Run SAM with dense prompt only\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# with torch.inference_mode():\n",
    "#     masks, scores, logits = predictor.predict(\n",
    "#         point_coords=input_point,\n",
    "#         point_labels=input_label,\n",
    "#         # box=None,\n",
    "#         # mask_input=mask_input,\n",
    "#         multimask_output=True\n",
    "#     )\n",
    "\n",
    "# for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.imshow(img_rgb)\n",
    "#     show_mask(mask, plt.gca())\n",
    "#     show_points(input_point, input_label, plt.gca())\n",
    "#     plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()  \n",
    "\n",
    "# best_mask = logits[np.argmax(scores), :, :]\n",
    "# print(f\"Best mask logit shape: {best_mask.shape}\")\n",
    "\n",
    "# # Run prediction again with best_mask as prompt\n",
    "# refined_mask, scores, logits = predictor.predict(\n",
    "#     point_coords=None,\n",
    "#     point_labels=None,\n",
    "#     box=None,\n",
    "#     multimask_output=False,\n",
    "#     mask_input=best_mask[None, :, :]\n",
    "# )\n",
    "\n",
    "# Run prediction with coarse mask as prompt\n",
    "binary_mask = cv2.resize(binary_mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "logit_mask = np.where(binary_mask > 0, 10.0, -10.0).astype(np.float32)\n",
    "print(np.max(logit_mask))\n",
    "output_mask_coarse, scores, logits = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=None,\n",
    "    multimask_output=False,\n",
    "    mask_input=logit_mask[None, :, :]\n",
    ")\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(output_mask_coarse[0, :, :], cmap='gray')\n",
    "plt.title(\"Coarse Mask Output\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# # # Refined mask (shape: 1024x1024)\n",
    "# # output_mask = masks[0,:,:].astype(np.uint8)\n",
    "# # print(f\"Output masks shape: {output_mask.shape}\")\n",
    "\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # # 5. Upsample refined mask to original image size and overlay\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # # H_orig, W_orig = img_rgb.shape[:2]\n",
    "# # # mask_fullres = cv2.resize(\n",
    "# # #     refined_mask,\n",
    "# # #     (W_orig, H_orig),\n",
    "# # #     interpolation=cv2.INTER_NEAREST\n",
    "# # # )\n",
    "\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # # 6. Visualize overlay\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# # plt.imshow(img_rgb)\n",
    "# plt.imshow(refined_mask[0, :, :], cmap='gray')\n",
    "# plt.title(\"SAM Output Mask\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cdf63",
   "metadata": {},
   "source": [
    "# Automatic mask generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.array([1.0, 0.0, 0.0, 0.35])  # Red color with 35% opacity\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "# Use SAM to automatically generate masks for image\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(img_rgb)\n",
    "\n",
    "print(\"Number of masks: \", len(masks))\n",
    "print(masks[0].keys())\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img_rgb)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show() \n",
    "\n",
    "# Visualize the mask with the best predicted_iou\n",
    "best_mask_ann = max(masks, key=lambda x: x['predicted_iou'])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_rgb)\n",
    "show_anns([best_mask_ann])\n",
    "plt.axis('off')\n",
    "plt.title(\"Best Mask with Highest Predicted IoU\")\n",
    "plt.show()\n",
    "\n",
    "for idx, mask_ann in enumerate(masks):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_rgb)\n",
    "    show_anns([mask_ann])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Mask {idx + 1} - Predicted IoU: {mask_ann['predicted_iou']:.3f}, Stability Score: {mask_ann['stability_score']:.3f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7310b",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b76d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def calibrate_intrinsics(image_glob, pattern_size=(9,6), square_size=0.025):\n",
    "    \"\"\"\n",
    "    Calibrate a single camera's intrinsics using checkerboard images.\n",
    "    \n",
    "    Args:\n",
    "        image_glob (str): Glob pattern for calibration images.\n",
    "        pattern_size (tuple): Number of inner corners per checkerboard row, col.\n",
    "        square_size (float): Size of a checkerboard square in meters.\n",
    "        \n",
    "    Returns:\n",
    "        ret (float): RMS re-projection error.\n",
    "        K (ndarray): Camera matrix (3x3).\n",
    "        dist (ndarray): Distortion coefficients.\n",
    "        rvecs, tvecs: Extrinsic vectors for each image.\n",
    "    \"\"\"\n",
    "    # Prepare object points (0,0,0), ..., scaled by square_size\n",
    "    objp = np.zeros((pattern_size[0]*pattern_size[1],3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "    objp *= square_size\n",
    "\n",
    "    objpoints, imgpoints = [], []\n",
    "    images = glob.glob(image_glob)\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\n",
    "        if not ret:\n",
    "            continue\n",
    "        corners2 = cv2.cornerSubPix(\n",
    "            gray, corners, (11,11), (-1,-1),\n",
    "            criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners2)\n",
    "\n",
    "    ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
    "        objpoints, imgpoints, gray.shape[::-1], None, None\n",
    "    )\n",
    "    print(f\"Intrinsics RMS error: {ret:.4f}\")\n",
    "    return ret, K, dist, rvecs, tvecs\n",
    "\n",
    "def stereo_calibrate(\n",
    "    glob1, glob2, pattern_size, square_size,\n",
    "    K1, d1, K2, d2,\n",
    "    flags=cv2.CALIB_FIX_INTRINSIC\n",
    "):\n",
    "    \"\"\"\n",
    "    Stereo calibrate two cameras that have already been intrinsically calibrated.\n",
    "    \n",
    "    Args:\n",
    "        glob1, glob2: Glob patterns for the two cameras' images.\n",
    "        pattern_size, square_size: Checkerboard parameters.\n",
    "        K1, d1, K2, d2: Intrinsics and distortions for camera 1 and 2.\n",
    "        flags: calibration flags.\n",
    "        \n",
    "    Returns:\n",
    "        R, T, E, F: Rotation, translation, essential and fundamental matrices.\n",
    "    \"\"\"\n",
    "    objp = np.zeros((pattern_size[0]*pattern_size[1],3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "    objp *= square_size\n",
    "\n",
    "    objpoints, imgpoints1, imgpoints2 = [], [], []\n",
    "    imgs1 = sorted(glob.glob(glob1))\n",
    "    imgs2 = sorted(glob.glob(glob2))\n",
    "    for f1, f2 in zip(imgs1, imgs2):\n",
    "        im1 = cv2.imread(f1); gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "        im2 = cv2.imread(f2); gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "        r1, c1 = cv2.findChessboardCorners(gray1, pattern_size, None)\n",
    "        r2, c2 = cv2.findChessboardCorners(gray2, pattern_size, None)\n",
    "        if not (r1 and r2):\n",
    "            continue\n",
    "        c1 = cv2.cornerSubPix(\n",
    "            gray1, c1, (11,11), (-1,-1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        c2 = cv2.cornerSubPix(\n",
    "            gray2, c2, (11,11), (-1,-1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        objpoints.append(objp)\n",
    "        imgpoints1.append(c1)\n",
    "        imgpoints2.append(c2)\n",
    "\n",
    "    ret, _, _, _, _, R, T, E, F = cv2.stereoCalibrate(\n",
    "        objpoints, imgpoints1, imgpoints2,\n",
    "        K1, d1, K2, d2,\n",
    "        gray1.shape[::-1],\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-5),\n",
    "        flags=flags\n",
    "    )\n",
    "    print(f\"Stereo RMS error: {ret:.4f}\")\n",
    "    return R, T, E, F\n",
    "\n",
    "def calibrate_camera_to_world(\n",
    "    image_glob, world_objpoints=None,\n",
    "    pattern_size=(9,6), square_size=0.025,\n",
    "    K=None, dist=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Solve PnP to get camera-to-world extrinsics using a known world marker.\n",
    "    \n",
    "    Args:\n",
    "        image_glob: Glob for images where the world marker is visible.\n",
    "        world_objpoints: (N×3) array of 3D points in world coords.\n",
    "        pattern_size, square_size: if using a checkerboard as your world frame.\n",
    "        K, dist: intrinsic calibration for this camera.\n",
    "        \n",
    "    Returns:\n",
    "        rvec, tvec: rotation and translation from world to camera.\n",
    "    \"\"\"\n",
    "    if world_objpoints is None:\n",
    "        wp = np.zeros((pattern_size[0]*pattern_size[1],3), np.float32)\n",
    "        wp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "        wp *= square_size\n",
    "        world_objpoints = wp\n",
    "\n",
    "    for fname in glob.glob(image_glob):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\n",
    "        if not ret:\n",
    "            continue\n",
    "        corners2 = cv2.cornerSubPix(\n",
    "            gray, corners, (11,11), (-1,-1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        ret, rvec, tvec = cv2.solvePnP(world_objpoints, corners2, K, dist)\n",
    "        if ret:\n",
    "            proj, _ = cv2.projectPoints(world_objpoints, rvec, tvec, K, dist)\n",
    "            err = np.linalg.norm(corners2.reshape(-1,2) - proj.reshape(-1,2)) / len(proj)\n",
    "            print(f\"SolvePnP reprojection error: {err*1e3:.2f} mm\")\n",
    "            return rvec, tvec\n",
    "    raise RuntimeError(\"Marker not detected in any image\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Intrinsics\n",
    "    _, K1, d1, _, _ = calibrate_intrinsics(\"calib_images/cam1/*.png\")\n",
    "    _, K2, d2, _, _ = calibrate_intrinsics(\"calib_images/cam2/*.png\")\n",
    "    \n",
    "    # 2) Stereo extrinsics\n",
    "    R, T, E, F = stereo_calibrate(\n",
    "        \"calib_images/cam1/*.png\", \"calib_images/cam2/*.png\",\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K1=K1, d1=d1, K2=K2, d2=d2\n",
    "    )\n",
    "    \n",
    "    # 3) Camera-to-world\n",
    "    rvec1, tvec1 = calibrate_camera_to_world(\n",
    "        \"marker_images/cam1/*.png\", None,\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K=K1, dist=d1\n",
    "    )\n",
    "    rvec2, tvec2 = calibrate_camera_to_world(\n",
    "        \"marker_images/cam2/*.png\", None,\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K=K2, dist=d2\n",
    "    )\n",
    "    \n",
    "    print(\"Calibration complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33248df1",
   "metadata": {},
   "source": [
    "# Calibration ChArUco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4223e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2.aruco as aruco\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "\n",
    "def calibrate_intrinsics_charuco(glob_pattern, charuco_board, aruco_dict,\n",
    "                                 min_markers=20):\n",
    "    all_corners, all_ids, img_size = [], [], None\n",
    "\n",
    "    for fname in glob.glob(glob_pattern):\n",
    "        img = cv2.imread(fname)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        if img_size is None:\n",
    "            img_size = img_gray.shape[::-1]\n",
    "\n",
    "        detector = aruco.CharucoDetector(charuco_board)\n",
    "        charuco_corners, charuco_ids, marker_corners, marker_ids = detector.detectBoard(img_gray)\n",
    "\n",
    "        all_corners.append(charuco_corners)\n",
    "        all_ids.append(charuco_ids)\n",
    "\n",
    "    # 3. Calibrate\n",
    "    ret, K, dist, _, _ = aruco.calibrateCameraCharuco(\n",
    "        charucoCorners=all_corners,\n",
    "        charucoIds=all_ids,\n",
    "        board=charuco_board,\n",
    "        imageSize=img_size,\n",
    "        cameraMatrix=None,\n",
    "        distCoeffs=None\n",
    "    )\n",
    "    print(f\"Charuco intrinsics RMS error: {ret:.4f}\")\n",
    "    return K, dist\n",
    "\n",
    "def stereo_calibrate_charuco(glob1, glob2, charuco_board, aruco_dict,\n",
    "                             K1, d1, K2, d2, flags=cv2.CALIB_FIX_INTRINSIC):\n",
    "    objpoints, imgpts1, imgpts2 = [], [], []\n",
    "    size = None\n",
    "\n",
    "    # Pair up images\n",
    "    for f1, f2 in zip(sorted(glob.glob(glob1)), sorted(glob.glob(glob2))):\n",
    "        im1, im2 = cv2.imread(f1), cv2.imread(f2)\n",
    "        g1, g2 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY), cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "        if size is None: size = g1.shape[::-1]\n",
    "\n",
    "        # Detect & interpolate\n",
    "        c1, id1, _ = aruco.detectMarkers(g1, aruco_dict)\n",
    "        c2, id2, _ = aruco.detectMarkers(g2, aruco_dict)\n",
    "        r1, cc1, cid1 = aruco.interpolateCornersCharuco(c1, id1, g1, charuco_board)\n",
    "        r2, cc2, cid2 = aruco.interpolateCornersCharuco(c2, id2, g2, charuco_board)\n",
    "        if r1 < 20 or r2 < 20: continue\n",
    "\n",
    "        # Build object points (same for both)\n",
    "        objp = charuco_board.chessboardCorners[cid1.flatten()]\n",
    "        objpoints.append(objp)\n",
    "        imgpts1.append(cc1)\n",
    "        imgpts2.append(cc2)\n",
    "\n",
    "    # Stereo calibrate\n",
    "    rms, _, _, _, _, R, T, E, F = cv2.stereoCalibrate(\n",
    "        objpoints, imgpts1, imgpts2,\n",
    "        K1, d1, K2, d2, size,\n",
    "        criteria=(cv2.TERM_CRITERIA_MAX_ITER|cv2.TERM_CRITERIA_EPS,100,1e-5),\n",
    "        flags=flags\n",
    "    )\n",
    "    print(f\"Stereo RMS error: {rms:.4f}\")\n",
    "    return R, T\n",
    "\n",
    "\n",
    "def camera_to_world_charuco(glob_pattern, charuco_board, aruco_dict, K, dist):\n",
    "    for fname in glob.glob(glob_pattern):\n",
    "        im = cv2.imread(fname); gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        # 1. Detect markers + ChArUco\n",
    "        markers, ids, _ = aruco.detectMarkers(gray, aruco_dict)\n",
    "        ret, cc, cid = aruco.interpolateCornersCharuco(markers, ids, gray, charuco_board)\n",
    "        if ret < 20: continue\n",
    "\n",
    "        # 2. Get the corresponding 3D object points\n",
    "        obj_pts = charuco_board.chessboardCorners[cid.flatten()]\n",
    "\n",
    "        # 3. SolvePnP\n",
    "        _, rvec, tvec = cv2.solvePnP(obj_pts, cc, K, dist)\n",
    "        # 4. (Optional) compute reprojection error\n",
    "        proj, _ = cv2.projectPoints(obj_pts, rvec, tvec, K, dist)\n",
    "        err = np.linalg.norm(cc.reshape(-1,2) - proj.reshape(-1,2)) / len(proj)\n",
    "        print(f\"Cam→World PnP error: {err*1000:.2f} mm\")\n",
    "        return rvec, tvec\n",
    "\n",
    "    raise RuntimeError(\"ChArUco board not found in any image\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define ChArUco board\n",
    "    square_size = 0.006\n",
    "    marker_length = 0.004\n",
    "    board_size = (10, 10)\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "    charuco_board = aruco.CharucoBoard(\n",
    "        size=board_size,\n",
    "        squareLength=square_size, \n",
    "        markerLength=marker_length,\n",
    "        dictionary=aruco_dict\n",
    "    )\n",
    "\n",
    "    # Calibrate instrinsices\n",
    "    K1, d1 = calibrate_intrinsics_charuco(\n",
    "        \"../camera_calibration/05-08-25/charuco_calib_images/cam0/*.png\", charuco_board, aruco_dict\n",
    "    )\n",
    "\n",
    "    # save calibration data\n",
    "    \n",
    "    print(\"Calibration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785abc0b",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def back_project(coord, camera_number):\n",
    "    \"\"\"\n",
    "    Back-project a 3D point to 2D pixel coordinates in the image plane.\n",
    "    \n",
    "    Args:\n",
    "        coord: 3D point in world coordinates.\n",
    "        camera_number: 0 for left camera, 1 for right camera.\n",
    "        \n",
    "    Returns:\n",
    "        pixel_coord: 2D pixel coordinates in the image plane.\n",
    "    \"\"\"\n",
    "    # Camera intrinsics and extrinsics (example values)\n",
    "    K = np.array([[1000, 0, 320],\n",
    "                  [0, 1000, 240],\n",
    "                  [0, 0, 1]])\n",
    "    R = np.eye(3) if camera_number == 0 else np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    T = np.array([0.5, 0, 0]) if camera_number == 1 else np.array([0, 0, 0])\n",
    "    \n",
    "    # Project the point\n",
    "    coord_homogeneous = np.append(coord, 1) # Convert to homogeneous coordinates\n",
    "    pixel_coord_homogeneous = K @ (R @ coord_homogeneous + T)\n",
    "    pixel_coord = pixel_coord_homogeneous[:2] / pixel_coord_homogeneous[2]\n",
    "    \n",
    "    return pixel_coord\n",
    "\n",
    "# 1) Create lookup table for voxel grid\n",
    "Nx, Ny, Nz = 20, 20, 20\n",
    "origin = np.array([0, 0, 0])\n",
    "voxel_size = np.array([0.01, 0.01, 0.01])\n",
    "lookup_table = np.zeros((Nx, Ny, Nz, 2, 2), dtype=np.float32)\n",
    "for x in range(Nx):\n",
    "    for y in range(Ny):\n",
    "        for z in range(Nz):\n",
    "            coord = origin + np.array([x, y, z]) * voxel_size\n",
    "            for camera_number in range(2):\n",
    "                pixel_coord = back_project(coord, camera_number)\n",
    "                lookup_table[x, y, z, camera_number] = np.reshape(pixel_coord, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Obtain masks for left and right cameras\n",
    "# TODO: Replace with actual image loading and processing\n",
    "mask_0 = np.zeros((480, 640), dtype=np.uint8)\n",
    "mask_1 = np.zeros((480, 640), dtype=np.uint8)\n",
    "\n",
    "# 3) Voxel carving: keep (x,y,z) if both silhouettes agree\n",
    "volume = np.zeros((Nx, Ny, Nz), dtype=np.uint8)\n",
    "for x in range(Nx):\n",
    "    for y in range(Ny):\n",
    "        for z in range(Nz):\n",
    "            pixel_coord_0 = lookup_table[x, y, z, 0]\n",
    "            pixel_coord_1 = lookup_table[x, y, z, 1]\n",
    "            if mask_0[pixel_coord_0[0], pixel_coord_0[1]] and \\\n",
    "               mask_1[pixel_coord_1[0], pixel_coord_1[1]]:\n",
    "                volume[x, y, z] = 1\n",
    "                \n",
    "points = np.argwhere(volume == 1)  # Get indices of carved voxels\n",
    "                \n",
    "\n",
    "# 4) Compute a simple centerline: for each z-slice, average x & y\n",
    "centerline = []\n",
    "for zk in z:\n",
    "    slice_pts = points[np.isclose(points[:, 2], zk)]\n",
    "    if len(slice_pts) > 0:\n",
    "        cx, cy = slice_pts[:,0].mean(), slice_pts[:,1].mean()\n",
    "        centerline.append([cx, cy, zk])\n",
    "centerline = np.array(centerline)\n",
    "\n",
    "# 5) Estimate tangent at the tip (last two points)\n",
    "v = centerline[-1] - centerline[-2]\n",
    "theta = np.arccos(v[2] / np.linalg.norm(v))  # bending magnitude\n",
    "phi   = np.arctan2(v[1], v[0])              # bending plane\n",
    "\n",
    "print(\"Estimated bending angle θ (rad):\", theta)\n",
    "print(\"Estimated bending plane φ (rad):\", phi)\n",
    "\n",
    "# 6) Plot a 2D x–z view of the carved voxels and the centerline\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(points[:,0], points[:,2], marker='.', label='voxels')\n",
    "ax.plot(centerline[:,0], centerline[:,2], label='centerline')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('z')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0475980",
   "metadata": {},
   "source": [
    "# Create Charuco board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28669331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import mm\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for the ChArUco board\n",
    "SQUARE_SIZE_MM = 6   # Desired square size in mm\n",
    "MARKER_SIZE_MM = 4   # Desired marker size in mm\n",
    "SQUARES_X = 10       # Number of squares along x-axis\n",
    "SQUARES_Y = 10       # Number of squares along y-axis\n",
    "MARGIN_MM = 10       # Margin around the board\n",
    "\n",
    "# Create the ArUco dictionary\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "\n",
    "# Define the board size as a tuple (number of squares in x and y directions)\n",
    "board_size = (SQUARES_X, SQUARES_Y)\n",
    "\n",
    "# Create the Charuco board using the updated constructor\n",
    "charuco_board = aruco.CharucoBoard(\n",
    "    size=board_size,\n",
    "    squareLength=SQUARE_SIZE_MM / 1000.0,  # in meters\n",
    "    markerLength=MARKER_SIZE_MM / 1000.0,  # in meters\n",
    "    dictionary=aruco_dict\n",
    ")\n",
    "\n",
    "# Generate the board image using OpenCV\n",
    "img_size = (SQUARES_X * 300, SQUARES_Y * 300)  # Higher resolution for print quality\n",
    "img = charuco_board.generateImage(img_size)\n",
    "\n",
    "# Save the board image as a PNG for reference\n",
    "cv2.imwrite(\"charuco_board.png\", img)\n",
    "\n",
    "# Calculate the final PDF size including margins\n",
    "pdf_width = (SQUARES_X * SQUARE_SIZE_MM) + (2 * MARGIN_MM)  # mm\n",
    "pdf_height = (SQUARES_Y * SQUARE_SIZE_MM) + (2 * MARGIN_MM)  # mm\n",
    "\n",
    "# Create a PDF using ReportLab with precise dimensions\n",
    "pdf_path = \"charuco_board_accurate.pdf\"\n",
    "c = canvas.Canvas(pdf_path, pagesize=(pdf_width * mm, pdf_height * mm))\n",
    "\n",
    "# Convert the OpenCV image to a format suitable for PDF\n",
    "_, buffer = cv2.imencode(\".png\", img)\n",
    "image_data = buffer.tobytes()\n",
    "\n",
    "# Calculate the exact position for centering the image\n",
    "x_offset = MARGIN_MM * mm\n",
    "y_offset = MARGIN_MM * mm\n",
    "image_width = (SQUARES_X * SQUARE_SIZE_MM) * mm\n",
    "image_height = (SQUARES_Y * SQUARE_SIZE_MM) * mm\n",
    "\n",
    "# Draw the ChArUco board image onto the PDF\n",
    "c.drawImage(\"charuco_board.png\", x_offset, y_offset, image_width, image_height)\n",
    "\n",
    "# Save the PDF\n",
    "c.showPage()\n",
    "c.save()\n",
    "print(f\"Saved accurate PDF: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b6f4b",
   "metadata": {},
   "source": [
    "# Pixel Color Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class PixelColorClassifier:\n",
    "    def __init__(self, model_type='logistic', color_space='HSV'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model_type: 'logistic' or 'naive_bayes'\n",
    "            color_space: 'HSV', 'RGB', or 'BGR'\n",
    "        \"\"\"\n",
    "        if model_type == 'logistic':\n",
    "            self.model = LogisticRegression(max_iter=200)\n",
    "        elif model_type == 'naive_bayes':\n",
    "            self.model = GaussianNB()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type. Choose 'logistic' or 'naive_bayes'.\")\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.color_space = color_space\n",
    "        self.trained = False\n",
    "\n",
    "    def _convert_color(self, image_bgr):\n",
    "        import cv2\n",
    "        if self.color_space == 'HSV':\n",
    "            return cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "        elif self.color_space == 'RGB':\n",
    "            return cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        elif self.color_space == 'BGR':\n",
    "            return image_bgr\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported color space\")\n",
    "\n",
    "    def fit(self, image_bgr, fg_mask, bg_mask, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Train the model using foreground and background binary masks.\n",
    "\n",
    "        Parameters:\n",
    "            image_bgr: Input image (BGR format)\n",
    "            fg_mask: Binary mask where foreground pixels are 255\n",
    "            bg_mask: Binary mask where background pixels are 255\n",
    "        \"\"\"\n",
    "        img_color = self._convert_color(image_bgr)\n",
    "        H, W, _ = img_color.shape\n",
    "        flat_img = img_color.reshape(-1, 3)\n",
    "\n",
    "        fg_indices = np.where(fg_mask.flatten() > 0)[0]\n",
    "        bg_indices = np.where(bg_mask.flatten() > 0)[0]\n",
    "\n",
    "        n_fg = min(max_samples, len(fg_indices))\n",
    "        n_bg = min(max_samples, len(bg_indices))\n",
    "\n",
    "        fg_sample = np.random.choice(fg_indices, n_fg, replace=False)\n",
    "        bg_sample = np.random.choice(bg_indices, n_bg, replace=False)\n",
    "\n",
    "        X = np.vstack((flat_img[fg_sample], flat_img[bg_sample]))\n",
    "        y = np.hstack((np.ones(n_fg), np.zeros(n_bg)))\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "        self.trained = True\n",
    "\n",
    "    def predict_proba(self, image_bgr):\n",
    "        \"\"\"\n",
    "        Predict foreground probability for each pixel.\n",
    "\n",
    "        Parameters:\n",
    "            image_bgr: Input image (BGR format)\n",
    "\n",
    "        Returns:\n",
    "            prob_map: 2D numpy array of foreground probabilities\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise RuntimeError(\"PixelColorClassifier must be trained with `fit()` before prediction.\")\n",
    "\n",
    "        img_color = self._convert_color(image_bgr)\n",
    "        H, W, _ = img_color.shape\n",
    "        flat_img = img_color.reshape(-1, 3)\n",
    "\n",
    "        proba = self.model.predict_proba(flat_img)[:, 1]\n",
    "        return proba.reshape(H, W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47629909",
   "metadata": {},
   "source": [
    "# Collect Intrinsic Calib Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def configure_camera(devices):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        # Build the commands to configure the camera\n",
    "        commands = [\n",
    "            f\"v4l2-ctl -d {device} -c focus_automatic_continuous=0\",\n",
    "            f\"v4l2-ctl -d {device} -c auto_exposure=3\",\n",
    "            f\"v4l2-ctl -d {device} -c focus_absolute=35\",\n",
    "            # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "            # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "            # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "            # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "            # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "            # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "            # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "        ]\n",
    "\n",
    "        for command in commands:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")\n",
    "\n",
    "# Camera settings\n",
    "camera_id = [0]\n",
    "devices = [f\"/dev/video{i}\" for i in camera_id] \n",
    "configure_camera(devices)\n",
    "output_dir = \"../camera_calibration/05-08-25/charuco_calib_imagescalib_images/cam1\"  # Directory to save images\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(camera_id[0], cv2.CAP_V4L2)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press SPACE to capture image, ESC to exit.\")\n",
    "\n",
    "image_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "\n",
    "    # Display the live feed\n",
    "    cv2.imshow(\"Camera Feed - Press SPACE to capture, ESC to exit\", frame)\n",
    "\n",
    "    # Keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Save image on SPACE key press\n",
    "    if key == ord(' '):\n",
    "        # Construct image filename\n",
    "        image_path = os.path.join(output_dir, f\"img_{image_count:03d}.png\")\n",
    "        # Save the frame as PNG\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        print(f\"Captured: {image_path}\")\n",
    "        image_count += 1\n",
    "\n",
    "    # Exit on ESC key press\n",
    "    elif key == 27:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "# Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd6370",
   "metadata": {},
   "source": [
    "# Collect Extrinsic Calib Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def configure_camera(devices):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        # Build the commands to configure the camera\n",
    "        commands = [\n",
    "            f\"v4l2-ctl -d {device} -c focus_automatic_continuous=0\",\n",
    "            f\"v4l2-ctl -d {device} -c auto_exposure=3\",\n",
    "            f\"v4l2-ctl -d {device} -c focus_absolute=35\",\n",
    "            # f\"v4l2-ctl -d {device} -c exposure_time_absolute=333\",\n",
    "            # f\"v4l2-ctl -d {device} -c gain=0\",\n",
    "            # f\"v4l2-ctl -d {device} -c white_balance_automatic=0\",\n",
    "            # f\"v4l2-ctl -d {device} -c white_balance_temperature=4675\",\n",
    "            # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "            # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "            # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "        ]\n",
    "\n",
    "        for command in commands:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")\n",
    "\n",
    "# Camera settings\n",
    "camera_id = 1\n",
    "devices = [f\"/dev/video0\"] \n",
    "configure_camera(devices)\n",
    "output_dir = f\"../camera_calibration/05-08-25/extrinsic_calib_images/cam{camera_id}\"  # Directory to save images\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press SPACE to capture image, ESC to exit.\")\n",
    "\n",
    "image_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "\n",
    "    # Display the live feed\n",
    "    cv2.imshow(\"Camera Feed - Press SPACE to capture, ESC to exit\", frame)\n",
    "\n",
    "    # Keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Save image on SPACE key press\n",
    "    if key == ord(' '):\n",
    "        # Construct image filename\n",
    "        image_path = os.path.join(output_dir, f\"img_{image_count:03d}.png\")\n",
    "        # Save the frame as PNG\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        print(f\"Captured: {image_path}\")\n",
    "        image_count += 1\n",
    "\n",
    "    # Exit on ESC key press\n",
    "    elif key == 27:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "# Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec951fee",
   "metadata": {},
   "source": [
    "# Find correct focus distance   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "\n",
    "def set_focus(value):\n",
    "    command = f\"v4l2-ctl -d /dev/video0 -c focus_absolute={value}\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "cap.set(cv2.CAP_PROP_AUTOFOCUS, 0)  # Disable autofocus\n",
    "\n",
    "print(\"Use the UP and DOWN arrow keys to adjust focus. Press ESC to exit.\")\n",
    "focus_value = 120  # Start with a mid-range value\n",
    "set_focus(focus_value)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read from camera.\")\n",
    "        break\n",
    "\n",
    "    # Display current focus value on the frame\n",
    "    cv2.putText(frame, f\"Focus: {focus_value}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Adjust Focus - Use UP/DOWN keys\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord('w'):  # Increase focus\n",
    "        focus_value = min(focus_value + 5, 250)\n",
    "        set_focus(focus_value)\n",
    "        print(f\"Focus set to: {focus_value}\")\n",
    "    elif key == ord('s'):  # Decrease focus\n",
    "        focus_value = max(focus_value - 5, 0)\n",
    "        set_focus(focus_value)\n",
    "        print(f\"Focus set to: {focus_value}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7088ba",
   "metadata": {},
   "source": [
    "# Capture simultaneous images from two webcams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Directory to save images\n",
    "output_dir = \"../stereo_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Open both webcams\n",
    "cap1 = cv2.VideoCapture(0, cv2.CAP_V4L2)  # First webcam at /dev/video0, top cam\n",
    "cap2 = cv2.VideoCapture(2, cv2.CAP_V4L2)  # Second webcam at /dev/video2, side cam\n",
    "\n",
    "# Set resolution and frame rate for both cameras\n",
    "# cap1.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "# cap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "# cap1.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "# cap2.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "# cap2.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "# cap2.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "if not cap1.isOpened() or not cap2.isOpened():\n",
    "    print(\"Error: One or both cameras could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press SPACE to capture simultaneous images, ESC to exit.\")\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    # Read frames from both cameras\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "\n",
    "    if not ret1 or not ret2:\n",
    "        print(\"Error: One or both frames could not be read.\")\n",
    "        break\n",
    "\n",
    "    # Display both camera feeds with timestamps\n",
    "    # timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")\n",
    "    # cv2.putText(frame1, f\"Cam1 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    # cv2.putText(frame2, f\"Cam2 - {timestamp}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine and display both frames\n",
    "    combined = cv2.hconcat([frame1, frame2])\n",
    "    cv2.imshow(\"Camera 0 (top) + Camera 2 (side)\", combined)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord(' '):  # Space key to capture images\n",
    "        # Save images with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        img1_path = f\"{output_dir}/cam0_{frame_count}.png\"\n",
    "        img2_path = f\"{output_dir}/cam1_{frame_count}.png\"\n",
    "        cv2.imwrite(img1_path, frame1)\n",
    "        cv2.imwrite(img2_path, frame2)\n",
    "        print(f\"Captured images:\\n - {img1_path}\\n - {img2_path}\")\n",
    "        frame_count += 1\n",
    "\n",
    "# Release both cameras and close windows\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fb168",
   "metadata": {},
   "source": [
    "# Camera-Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Relative Rotation (R21):\n",
      " [[ 0.00441847  0.02945237 -0.99954709]\n",
      " [-0.05544742  0.99801003  0.0291781 ]\n",
      " [ 0.9984277   0.05528752  0.0060255 ]]\n",
      "Averaged Relative Translation (T21):\n",
      " [[0.17086164]\n",
      " [0.00920667]\n",
      " [0.22905064]]\n",
      "Relative transformation saved to 'relative_transformation.npz'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "def detect_charuco_corners(image_path, board, detector):\n",
    "    \"\"\"Detects Charuco corners using OpenCV 4.11+ functions.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Aruco markers and Charuco corners\n",
    "    charuco_corners, charuco_ids, _, _ = detector.detectBoard(gray)\n",
    "\n",
    "    if charuco_ids is not None and len(charuco_ids) > 4:\n",
    "        return charuco_corners, charuco_ids\n",
    "    return None, None\n",
    "\n",
    "def calculate_relative_transform(R1, T1, R2, T2):\n",
    "    \"\"\"Calculate the relative rotation and translation between two cameras.\n",
    "    Returns R21 and T21, the rotation and translation of camera 2 in camera 1's frame.\"\"\"\n",
    "    R21 = np.dot(R2, R1.T)\n",
    "    T21 = T2 - np.dot(R21, T1)\n",
    "    return R21, T21\n",
    "\n",
    "def get_extrinsics(image_path, K, dist, board, detector):\n",
    "    \"\"\"Compute extrinsics for a single image.\"\"\"\n",
    "    charuco_corners, charuco_ids = detect_charuco_corners(image_path, board, detector)\n",
    "    if charuco_corners is not None:\n",
    "        obj_points = board.getChessboardCorners()[charuco_ids.flatten()]\n",
    "        ret, rvec, tvec = cv2.solvePnP(obj_points, charuco_corners, K, dist)\n",
    "        if ret:\n",
    "            R, _ = cv2.Rodrigues(rvec)\n",
    "            return R, tvec\n",
    "    return None, None\n",
    "\n",
    "# Load intrinsic parameters\n",
    "calib_data_file = \"../camera_calibration/05-08-25/camera_calib_data.pkl\"\n",
    "with open(calib_data_file, 'rb') as f:\n",
    "    calib_data = pickle.load(f)\n",
    "\n",
    "K1 = calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist1 = calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "K2 = calib_data[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "dist2 = calib_data[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "\n",
    "# Create Aruco dictionary and Charuco board\n",
    "# Parameters for the ChArUco board\n",
    "SQUARE_SIZE_MM = 6   # Desired square size in mm\n",
    "MARKER_SIZE_MM = 4   # Desired marker size in mm\n",
    "SQUARES_X = 10       # Number of squares along x-axis\n",
    "SQUARES_Y = 10       # Number of squares along y-axis\n",
    "\n",
    "# Create the ArUco dictionary\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "\n",
    "# Define the board size as a tuple (number of squares in x and y directions)\n",
    "board_size = (SQUARES_X, SQUARES_Y)\n",
    "\n",
    "# Create the Charuco board using the updated constructor\n",
    "charuco_board = aruco.CharucoBoard(\n",
    "    size=board_size,\n",
    "    squareLength=SQUARE_SIZE_MM / 1000.0,  # in meters\n",
    "    markerLength=MARKER_SIZE_MM / 1000.0,  # in meters\n",
    "    dictionary=aruco_dict\n",
    ")\n",
    "\n",
    "# Create the Aruco and Charuco detector objects (new in OpenCV 4.11+)\n",
    "aruco_detector = aruco.ArucoDetector(aruco_dict)\n",
    "charuco_detector = aruco.CharucoDetector(charuco_board)\n",
    "\n",
    "# Paths to synchronized images\n",
    "cam1_images = sorted(glob.glob(\"../stereo_images/cam0_*.png\"))\n",
    "cam2_images = sorted(glob.glob(\"../stereo_images/cam1_*.png\"))\n",
    "\n",
    "# Arrays to store transformations\n",
    "relative_rotations = []\n",
    "relative_translations = []\n",
    "\n",
    "for img1_path, img2_path in zip(cam1_images, cam2_images):\n",
    "    # Get extrinsics for both cameras using updated functions\n",
    "    R1, T1 = get_extrinsics(img1_path, K1, dist1, charuco_board, charuco_detector)\n",
    "    R2, T2 = get_extrinsics(img2_path, K2, dist2, charuco_board, charuco_detector)\n",
    "\n",
    "    if R1 is not None and R2 is not None:\n",
    "        # Calculate the relative transformation between cameras\n",
    "        R21, T21 = calculate_relative_transform(R1, T1, R2, T2)\n",
    "        relative_rotations.append(R21)\n",
    "        relative_translations.append(T21)\n",
    "\n",
    "# Average the transformations\n",
    "R_avg = sum(relative_rotations) / len(relative_rotations)\n",
    "T_avg = sum(relative_translations) / len(relative_translations)\n",
    "\n",
    "# Print the averaged relative transformation\n",
    "print(\"Averaged Relative Rotation (R21):\\n\", R_avg)\n",
    "print(\"Averaged Relative Translation (T21):\\n\", T_avg)\n",
    "\n",
    "# Save the relative transformation\n",
    "np.savez(\"relative_transformation.npz\", R21=R_avg, T21=T_avg)\n",
    "print(\"Relative transformation saved to 'relative_transformation.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c28c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation Angle: 89.76 degrees\n",
      "Rotation Axis: [ 0.01305083 -0.99901324 -0.04245269]\n",
      "Relative Translation (T21): [0.1709 0.0092 0.2291]\n",
      "Flipped Translation (T12): [-0.22897572 -0.02689238  0.16917131]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rotation_angle(R):\n",
    "    \"\"\"Calculate the rotation angle from the rotation matrix.\"\"\"\n",
    "    trace = np.trace(R)\n",
    "    theta = np.arccos((trace - 1) / 2)\n",
    "    return np.degrees(theta)\n",
    "\n",
    "def rotation_axis(R):\n",
    "    \"\"\"Calculate the axis of rotation from the rotation matrix.\"\"\"\n",
    "    angle = rotation_angle(R)\n",
    "    axis = np.array([\n",
    "        R[2, 1] - R[1, 2],\n",
    "        R[0, 2] - R[2, 0],\n",
    "        R[1, 0] - R[0, 1]\n",
    "    ])\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    return axis, angle\n",
    "\n",
    "# Relative Rotation Matrix (from your calibration)\n",
    "R21 = np.array([\n",
    "    [0.0044, 0.0295, -0.9995],\n",
    "    [-0.0554, 0.9980, 0.0292],\n",
    "    [0.9984, 0.0553, 0.0060]\n",
    "])\n",
    "\n",
    "# Relative Translation Vector (from your calibration)\n",
    "T21 = np.array([0.1709, 0.0092, 0.2291])\n",
    "\n",
    "# Calculate the rotation angle and axis\n",
    "axis, angle = rotation_axis(R21)\n",
    "\n",
    "print(f\"Rotation Angle: {angle:.2f} degrees\")\n",
    "print(f\"Rotation Axis: {axis}\")\n",
    "\n",
    "# Interpretation of Translation\n",
    "print(f\"Relative Translation (T21): {T21}\")\n",
    "\n",
    "# Flip the interpretation to see if the reverse makes sense\n",
    "T12 = -np.dot(R21.T, T21)\n",
    "print(f\"Flipped Translation (T12): {T12}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fbed4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320043e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "def project_points(points_3d, R, T, K, dist):\n",
    "    \"\"\"Projects 3D points to 2D image plane using extrinsic and intrinsic parameters.\"\"\"\n",
    "    projected_points, _ = cv2.projectPoints(points_3d, cv2.Rodrigues(R)[0], T, K, dist)\n",
    "    return projected_points.squeeze()\n",
    "\n",
    "def draw_points(image, points, color, label):\n",
    "    \"\"\"Draws points on the image.\"\"\"\n",
    "    for point in points:\n",
    "        cv2.circle(image, tuple(point.astype(int)), 5, color, -1)\n",
    "    # Label the points for clarity\n",
    "    cv2.putText(image, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "def visualize_relative_transformation(img1_path, img2_path, R1, T1, R2, T2, K1, dist1, K2, dist2, R21, T21):\n",
    "    \"\"\"Visualize relative transformation between two cameras.\"\"\"\n",
    "    # Load the images\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "\n",
    "    # Define a set of 3D points around the origin (world frame)\n",
    "    points_3d = np.array([\n",
    "        [0, 0, 0],       # Origin\n",
    "        [0.05, 0, 0],    # X-axis point\n",
    "        [0, 0.05, 0],    # Y-axis point\n",
    "        [0, 0, 0.05],    # Z-axis point\n",
    "        [0.05, 0.05, 0], # XY-plane point\n",
    "        [0.05, 0, 0.05], # XZ-plane point\n",
    "        [0, 0.05, 0.05], # YZ-plane point\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Project points from world frame to camera 0 frame\n",
    "    projected_cam1 = project_points(points_3d, R1, T1, K1, dist1)\n",
    "\n",
    "    # Transform the points to the second camera using the relative transformation\n",
    "    transformed_points = (R21 @ points_3d.T).T + T21.T\n",
    "    projected_cam2 = project_points(transformed_points, R2, T2, K2, dist2)\n",
    "\n",
    "    # Draw both detected and projected points on both images\n",
    "    draw_points(img1, projected_cam1, (0, 255, 0), \"Camera 0: Projected\")  # Green: Projected on Cam0\n",
    "    draw_points(img2, projected_cam2, (255, 0, 0), \"Camera 1: Projected\")  # Blue: Projected on Cam1\n",
    "\n",
    "    # Combine the images for side-by-side visualization\n",
    "    combined = cv2.hconcat([img1, img2])\n",
    "    cv2.imshow(\"Calibration Verification: Camera 0 (Left) | Camera 1 (Right)\", combined)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load the relative transformation between cameras\n",
    "rel_data = np.load(\"relative_transformation.npz\")\n",
    "R21 = rel_data['R21']\n",
    "T21 = rel_data['T21']\n",
    "\n",
    "# Load intrinsic parameters\n",
    "calib_data_file = \"../camera_calibration/05-08-25/camera_calib_data.pkl\"\n",
    "with open(calib_data_file, 'rb') as f:\n",
    "    calib_data = pickle.load(f)\n",
    "\n",
    "K1 = calib_data[\"cam0\"][\"intrinsics\"][\"K\"]\n",
    "dist1 = calib_data[\"cam0\"][\"intrinsics\"][\"d\"]\n",
    "K2 = calib_data[\"cam1\"][\"intrinsics\"][\"K\"]\n",
    "dist2 = calib_data[\"cam1\"][\"intrinsics\"][\"d\"]\n",
    "\n",
    "# Paths to synchronized images\n",
    "cam1_images = sorted(glob.glob(\"../stereo_images/cam0_*.png\"))\n",
    "cam2_images = sorted(glob.glob(\"../stereo_images/cam1_*.png\"))\n",
    "\n",
    "# Load the first image pair for visualization\n",
    "img1_path = cam1_images[0]\n",
    "img2_path = cam2_images[0]\n",
    "\n",
    "# Load the pre-computed extrinsics for both cameras\n",
    "R1, T1 = get_extrinsics(img1_path, K1, dist1, charuco_board, charuco_detector)\n",
    "R2, T2 = get_extrinsics(img2_path, K2, dist2, charuco_board, charuco_detector)\n",
    "\n",
    "# Visualize the relative transformation using the first image pair\n",
    "visualize_relative_transformation(img1_path, img2_path, R1, T1, R2, T2, K1, dist1, K2, dist2, R21, T21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798a18b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30dd4160",
   "metadata": {},
   "source": [
    "## Camera-World Calibration with Aruco Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128db1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture image for camera-world calibration\n",
    "import cv2\n",
    "\n",
    "camera_id = 0 # use top camera\n",
    "devices = [f\"/dev/video0\"]\n",
    "configure_camera(devices)\n",
    "output_dir = \"../camera_calibration/05-08-25/camera_world_calib_images/cam0\"  # Directory to save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate camera to world frame transform using aruco marker\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
