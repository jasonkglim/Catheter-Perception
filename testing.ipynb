{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c855375d",
   "metadata": {},
   "source": [
    "# SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "image_bgr = cv2.imread(\"images/frame_20250429_183115.png\")\n",
    "if image_bgr is None:\n",
    "    raise RuntimeError(\"Could not load image.\")\n",
    "\n",
    "image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "image_display = image_bgr.copy()\n",
    "clicked_points = []\n",
    "\n",
    "# Mouse callback function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and len(clicked_points) < 2:\n",
    "        hsv_value = image_hsv[y, x]\n",
    "        clicked_points.append((x, y, hsv_value))\n",
    "\n",
    "        # Draw a small cross\n",
    "        cv2.drawMarker(image_display, (x, y), (0, 0, 255), cv2.MARKER_CROSS, 10, 2)\n",
    "        cv2.imshow(\"Click to sample HSV\", image_display)\n",
    "\n",
    "        print(f\"Clicked point {len(clicked_points)}: x={x}, y={y}, HSV={hsv_value}\")\n",
    "\n",
    "        if len(clicked_points) == 2:\n",
    "            print(\"\\nDone! Use these HSV values to tune your threshold.\")\n",
    "            print(f\"Background HSV: {clicked_points[0][2]}\")\n",
    "            print(f\"Foreground HSV: {clicked_points[1][2]}\")\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "# Display the image and wait for two clicks\n",
    "cv2.imshow(\"Click to sample HSV\", image_display)\n",
    "cv2.setMouseCallback(\"Click to sample HSV\", on_mouse)\n",
    "print(\"Click once on the BACKGROUND, then once on the CATHETER TIP.\")\n",
    "\n",
    "input_point = np.array([clicked_points[0][:2], clicked_points[1][:2]])\n",
    "input_label = np.array([0, 1]) # 0 for background, 1 for foreground\n",
    "print(\"Input point:\", input_point)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50f5ac",
   "metadata": {},
   "source": [
    "# SAM model and image set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2caa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Load SAM ViT-B on CPU\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "checkpoint_path = \"/home/arclab/repos/segment-anything/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "sam.to(\"cuda\")\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Load image and create coarse foreground mask (non-green areas)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "img_bgr = cv2.imread(\"images/frame_20250429_183115.png\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "img_hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "print(\"Original image shape:\", img_bgr.shape)\n",
    "\n",
    "\n",
    "lower_green = np.array([35, 30, 100])\n",
    "upper_green = np.array([85, 255, 255])\n",
    "green_mask = cv2.inRange(img_hsv, lower_green, upper_green)\n",
    "foreground_mask = cv2.bitwise_not(green_mask)\n",
    "binary_mask = np.where(foreground_mask > 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "if img_bgr is None:\n",
    "    raise RuntimeError(\"Could not load image.\")\n",
    "\n",
    "predictor.set_image(img_rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381587a3",
   "metadata": {},
   "source": [
    "# Automated point and box prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d257b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# ============ APPROACH 1: POINT PROMPT =========\n",
    "# ===============================================\n",
    "# Get the centroid of the binary mask\n",
    "M = cv2.moments(binary_mask)\n",
    "if M[\"m00\"] != 0:\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "    point_coords = np.array([[cx, cy]])\n",
    "else:\n",
    "    raise ValueError(\"No foreground pixels found in binary mask\")\n",
    "\n",
    "print(\"Centroid coordinates:\", cx, cy)\n",
    "\n",
    "\n",
    "point_labels = np.array([1])  # 1 = foreground\n",
    "\n",
    "masks_point, scores_point, _ = predictor.predict(\n",
    "    point_coords=point_coords,\n",
    "    point_labels=point_labels,\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# ============ APPROACH 2: BOX PROMPT ===========\n",
    "# ===============================================\n",
    "x, y, w, h = cv2.boundingRect(binary_mask)\n",
    "box = np.array([[x, y, x + w, y + h]])\n",
    "\n",
    "masks_box, scores_box, _ = predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=False\n",
    ")\n",
    "\n",
    "# ---- Visualize Results ----\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axs[0].imshow(image_rgb)\n",
    "axs[0].set_title(\"Original Image\")\n",
    "\n",
    "axs[1].imshow(image_rgb)\n",
    "axs[1].imshow(masks_point[0], alpha=0.5, cmap='Reds')\n",
    "axs[1].scatter(cx, cy, c='blue', s=40)\n",
    "axs[1].set_title(\"Point Prompt Result\")\n",
    "\n",
    "axs[2].imshow(image_rgb)\n",
    "axs[2].imshow(masks_box[0], alpha=0.5, cmap='Reds')\n",
    "axs[2].add_patch(plt.Rectangle((x, y), w, h, edgecolor='blue', facecolor='none', lw=2))\n",
    "axs[2].set_title(\"Box Prompt Result\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_rgb)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define green background HSV range\n",
    "lower_green = np.array([35, 30, 100])\n",
    "upper_green = np.array([85, 255, 255])\n",
    "green_mask = cv2.inRange(img_hsv, lower_green, upper_green)\n",
    "print(\"max val: \", np.max(green_mask))\n",
    "print(\"min val: \", np.min(green_mask))\n",
    "\n",
    "# Invert: 1 for non-green (catheter), 0 for background\n",
    "foreground_mask = cv2.bitwise_not(green_mask)\n",
    "print(\"max val: \", np.max(foreground_mask))\n",
    "\n",
    "# Optionally clean up mask\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_OPEN, kernel)\n",
    "print(\"max val: \", np.max(foreground_mask))\n",
    "# Convert to binary mask\n",
    "binary_mask = np.where(foreground_mask > 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "print(\"min val: \", np.min(binary_mask))\n",
    "print(\"max val: \", np.max(binary_mask))\n",
    "print(\"binary mask shape: \", binary_mask.shape)\n",
    "# Visualize foreground mask\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(binary_mask, cmap='gray')\n",
    "plt.title(\"Foreground Mask (Non-Green Areas)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9345d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Resize the coarse mask to SAM's expected prompt input resolution\n",
    "# # For ViT-B: patch size = 16 → input image (1024) / 16 = 64 → 1/4 = 16x16\n",
    "# prompt_H, prompt_W = 256, 256\n",
    "\n",
    "# coarse_mask = cv2.resize(\n",
    "#     coarse_mask.astype(np.float32),\n",
    "#     (prompt_W, prompt_H),\n",
    "#     interpolation=cv2.INTER_NEAREST\n",
    "# )\n",
    "\n",
    "# print(\"Coarse mask shape:\", coarse_mask.shape)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(coarse_mask)\n",
    "# plt.title(\"Coarse Mask (Non-Green Areas)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # 3. Feed image to SAM\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# image_embedding = predictor.get_image_embedding().cpu().numpy()\n",
    "\n",
    "# print(\"Image embedding shape:\", image_embedding.shape)\n",
    "# print(\"Sample values:\", image_embedding[0, :, :5, :5])  # Print part of it for inspection\n",
    "\n",
    "\n",
    "# # # Add channel dimension → (1, 16, 16)\n",
    "# # mask_input = mask_resized[np.newaxis, :, :]\n",
    "# # print(f\"Mask input shape: {mask_input.shape}\")\n",
    "\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # 4. Run SAM with dense prompt only\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# with torch.inference_mode():\n",
    "#     masks, scores, logits = predictor.predict(\n",
    "#         point_coords=input_point,\n",
    "#         point_labels=input_label,\n",
    "#         # box=None,\n",
    "#         # mask_input=mask_input,\n",
    "#         multimask_output=True\n",
    "#     )\n",
    "\n",
    "# for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.imshow(img_rgb)\n",
    "#     show_mask(mask, plt.gca())\n",
    "#     show_points(input_point, input_label, plt.gca())\n",
    "#     plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()  \n",
    "\n",
    "# best_mask = logits[np.argmax(scores), :, :]\n",
    "# print(f\"Best mask logit shape: {best_mask.shape}\")\n",
    "\n",
    "# # Run prediction again with best_mask as prompt\n",
    "# refined_mask, scores, logits = predictor.predict(\n",
    "#     point_coords=None,\n",
    "#     point_labels=None,\n",
    "#     box=None,\n",
    "#     multimask_output=False,\n",
    "#     mask_input=best_mask[None, :, :]\n",
    "# )\n",
    "\n",
    "# Run prediction with coarse mask as prompt\n",
    "binary_mask = cv2.resize(binary_mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "logit_mask = np.where(binary_mask > 0, 10.0, -10.0).astype(np.float32)\n",
    "print(np.max(logit_mask))\n",
    "output_mask_coarse, scores, logits = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=None,\n",
    "    multimask_output=False,\n",
    "    mask_input=logit_mask[None, :, :]\n",
    ")\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(output_mask_coarse[0, :, :], cmap='gray')\n",
    "plt.title(\"Coarse Mask Output\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# # # Refined mask (shape: 1024x1024)\n",
    "# # output_mask = masks[0,:,:].astype(np.uint8)\n",
    "# # print(f\"Output masks shape: {output_mask.shape}\")\n",
    "\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # # 5. Upsample refined mask to original image size and overlay\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # # H_orig, W_orig = img_rgb.shape[:2]\n",
    "# # # mask_fullres = cv2.resize(\n",
    "# # #     refined_mask,\n",
    "# # #     (W_orig, H_orig),\n",
    "# # #     interpolation=cv2.INTER_NEAREST\n",
    "# # # )\n",
    "\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # # 6. Visualize overlay\n",
    "# # # ──────────────────────────────────────────────────────────────────────────────\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# # plt.imshow(img_rgb)\n",
    "# plt.imshow(refined_mask[0, :, :], cmap='gray')\n",
    "# plt.title(\"SAM Output Mask\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cdf63",
   "metadata": {},
   "source": [
    "# Automatic mask generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.array([1.0, 0.0, 0.0, 0.35])  # Red color with 35% opacity\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "# Use SAM to automatically generate masks for image\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(img_rgb)\n",
    "\n",
    "print(\"Number of masks: \", len(masks))\n",
    "print(masks[0].keys())\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img_rgb)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show() \n",
    "\n",
    "# Visualize the mask with the best predicted_iou\n",
    "best_mask_ann = max(masks, key=lambda x: x['predicted_iou'])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_rgb)\n",
    "show_anns([best_mask_ann])\n",
    "plt.axis('off')\n",
    "plt.title(\"Best Mask with Highest Predicted IoU\")\n",
    "plt.show()\n",
    "\n",
    "for idx, mask_ann in enumerate(masks):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_rgb)\n",
    "    show_anns([mask_ann])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Mask {idx + 1} - Predicted IoU: {mask_ann['predicted_iou']:.3f}, Stability Score: {mask_ann['stability_score']:.3f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7310b",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b76d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def calibrate_intrinsics(image_glob, pattern_size=(9,6), square_size=0.025):\n",
    "    \"\"\"\n",
    "    Calibrate a single camera's intrinsics using checkerboard images.\n",
    "    \n",
    "    Args:\n",
    "        image_glob (str): Glob pattern for calibration images.\n",
    "        pattern_size (tuple): Number of inner corners per checkerboard row, col.\n",
    "        square_size (float): Size of a checkerboard square in meters.\n",
    "        \n",
    "    Returns:\n",
    "        ret (float): RMS re-projection error.\n",
    "        K (ndarray): Camera matrix (3x3).\n",
    "        dist (ndarray): Distortion coefficients.\n",
    "        rvecs, tvecs: Extrinsic vectors for each image.\n",
    "    \"\"\"\n",
    "    # Prepare object points (0,0,0), ..., scaled by square_size\n",
    "    objp = np.zeros((pattern_size[0]*pattern_size[1],3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "    objp *= square_size\n",
    "\n",
    "    objpoints, imgpoints = [], []\n",
    "    images = glob.glob(image_glob)\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\n",
    "        if not ret:\n",
    "            continue\n",
    "        corners2 = cv2.cornerSubPix(\n",
    "            gray, corners, (11,11), (-1,-1),\n",
    "            criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners2)\n",
    "\n",
    "    ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
    "        objpoints, imgpoints, gray.shape[::-1], None, None\n",
    "    )\n",
    "    print(f\"Intrinsics RMS error: {ret:.4f}\")\n",
    "    return ret, K, dist, rvecs, tvecs\n",
    "\n",
    "def stereo_calibrate(\n",
    "    glob1, glob2, pattern_size, square_size,\n",
    "    K1, d1, K2, d2,\n",
    "    flags=cv2.CALIB_FIX_INTRINSIC\n",
    "):\n",
    "    \"\"\"\n",
    "    Stereo calibrate two cameras that have already been intrinsically calibrated.\n",
    "    \n",
    "    Args:\n",
    "        glob1, glob2: Glob patterns for the two cameras' images.\n",
    "        pattern_size, square_size: Checkerboard parameters.\n",
    "        K1, d1, K2, d2: Intrinsics and distortions for camera 1 and 2.\n",
    "        flags: calibration flags.\n",
    "        \n",
    "    Returns:\n",
    "        R, T, E, F: Rotation, translation, essential and fundamental matrices.\n",
    "    \"\"\"\n",
    "    objp = np.zeros((pattern_size[0]*pattern_size[1],3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "    objp *= square_size\n",
    "\n",
    "    objpoints, imgpoints1, imgpoints2 = [], [], []\n",
    "    imgs1 = sorted(glob.glob(glob1))\n",
    "    imgs2 = sorted(glob.glob(glob2))\n",
    "    for f1, f2 in zip(imgs1, imgs2):\n",
    "        im1 = cv2.imread(f1); gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "        im2 = cv2.imread(f2); gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "        r1, c1 = cv2.findChessboardCorners(gray1, pattern_size, None)\n",
    "        r2, c2 = cv2.findChessboardCorners(gray2, pattern_size, None)\n",
    "        if not (r1 and r2):\n",
    "            continue\n",
    "        c1 = cv2.cornerSubPix(\n",
    "            gray1, c1, (11,11), (-1,-1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        c2 = cv2.cornerSubPix(\n",
    "            gray2, c2, (11,11), (-1,-1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        objpoints.append(objp)\n",
    "        imgpoints1.append(c1)\n",
    "        imgpoints2.append(c2)\n",
    "\n",
    "    ret, _, _, _, _, R, T, E, F = cv2.stereoCalibrate(\n",
    "        objpoints, imgpoints1, imgpoints2,\n",
    "        K1, d1, K2, d2,\n",
    "        gray1.shape[::-1],\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-5),\n",
    "        flags=flags\n",
    "    )\n",
    "    print(f\"Stereo RMS error: {ret:.4f}\")\n",
    "    return R, T, E, F\n",
    "\n",
    "def calibrate_camera_to_world(\n",
    "    image_glob, world_objpoints=None,\n",
    "    pattern_size=(9,6), square_size=0.025,\n",
    "    K=None, dist=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Solve PnP to get camera-to-world extrinsics using a known world marker.\n",
    "    \n",
    "    Args:\n",
    "        image_glob: Glob for images where the world marker is visible.\n",
    "        world_objpoints: (N×3) array of 3D points in world coords.\n",
    "        pattern_size, square_size: if using a checkerboard as your world frame.\n",
    "        K, dist: intrinsic calibration for this camera.\n",
    "        \n",
    "    Returns:\n",
    "        rvec, tvec: rotation and translation from world to camera.\n",
    "    \"\"\"\n",
    "    if world_objpoints is None:\n",
    "        wp = np.zeros((pattern_size[0]*pattern_size[1],3), np.float32)\n",
    "        wp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "        wp *= square_size\n",
    "        world_objpoints = wp\n",
    "\n",
    "    for fname in glob.glob(image_glob):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\n",
    "        if not ret:\n",
    "            continue\n",
    "        corners2 = cv2.cornerSubPix(\n",
    "            gray, corners, (11,11), (-1,-1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        ret, rvec, tvec = cv2.solvePnP(world_objpoints, corners2, K, dist)\n",
    "        if ret:\n",
    "            proj, _ = cv2.projectPoints(world_objpoints, rvec, tvec, K, dist)\n",
    "            err = np.linalg.norm(corners2.reshape(-1,2) - proj.reshape(-1,2)) / len(proj)\n",
    "            print(f\"SolvePnP reprojection error: {err*1e3:.2f} mm\")\n",
    "            return rvec, tvec\n",
    "    raise RuntimeError(\"Marker not detected in any image\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Intrinsics\n",
    "    _, K1, d1, _, _ = calibrate_intrinsics(\"calib_images/cam1/*.png\")\n",
    "    _, K2, d2, _, _ = calibrate_intrinsics(\"calib_images/cam2/*.png\")\n",
    "    \n",
    "    # 2) Stereo extrinsics\n",
    "    R, T, E, F = stereo_calibrate(\n",
    "        \"calib_images/cam1/*.png\", \"calib_images/cam2/*.png\",\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K1=K1, d1=d1, K2=K2, d2=d2\n",
    "    )\n",
    "    \n",
    "    # 3) Camera-to-world\n",
    "    rvec1, tvec1 = calibrate_camera_to_world(\n",
    "        \"marker_images/cam1/*.png\", None,\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K=K1, dist=d1\n",
    "    )\n",
    "    rvec2, tvec2 = calibrate_camera_to_world(\n",
    "        \"marker_images/cam2/*.png\", None,\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K=K2, dist=d2\n",
    "    )\n",
    "    \n",
    "    print(\"Calibration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33248df1",
   "metadata": {},
   "source": [
    "# Calibration ChArUco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4223e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2.aruco as aruco\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def calibrate_intrinsics_charuco(glob_pattern, charuco_board, aruco_dict,\n",
    "                                 min_markers=20):\n",
    "    all_corners, all_ids, img_size = [], [], None\n",
    "\n",
    "    for fname in glob.glob(glob_pattern):\n",
    "        im = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        if img_size is None:\n",
    "            img_size = gray.shape[::-1]\n",
    "\n",
    "        # 1. Detect ArUco markers\n",
    "        corners, ids, _ = aruco.detectMarkers(gray, aruco_dict)\n",
    "        if ids is None or len(ids) < min_markers:\n",
    "            continue\n",
    "\n",
    "        # 2. Interpolate ChArUco corners\n",
    "        ret, charuco_corners, charuco_ids = aruco.interpolateCornersCharuco(\n",
    "            markerCorners=corners, markerIds=ids,\n",
    "            image=gray, board=charuco_board\n",
    "        )\n",
    "        if ret < 20:  # need at least N charuco corners\n",
    "            continue\n",
    "\n",
    "        all_corners.append(charuco_corners)\n",
    "        all_ids.append(charuco_ids)\n",
    "\n",
    "    # 3. Calibrate\n",
    "    ret, K, dist, _, _ = aruco.calibrateCameraCharuco(\n",
    "        charucoCorners=all_corners,\n",
    "        charucoIds=all_ids,\n",
    "        board=charuco_board,\n",
    "        imageSize=img_size,\n",
    "        cameraMatrix=None,\n",
    "        distCoeffs=None\n",
    "    )\n",
    "    print(f\"Charuco intrinsics RMS error: {ret:.4f}\")\n",
    "    return K, dist\n",
    "\n",
    "def stereo_calibrate_charuco(glob1, glob2, charuco_board, aruco_dict,\n",
    "                             K1, d1, K2, d2, flags=cv2.CALIB_FIX_INTRINSIC):\n",
    "    objpoints, imgpts1, imgpts2 = [], [], []\n",
    "    size = None\n",
    "\n",
    "    # Pair up images\n",
    "    for f1, f2 in zip(sorted(glob.glob(glob1)), sorted(glob.glob(glob2))):\n",
    "        im1, im2 = cv2.imread(f1), cv2.imread(f2)\n",
    "        g1, g2 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY), cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "        if size is None: size = g1.shape[::-1]\n",
    "\n",
    "        # Detect & interpolate\n",
    "        c1, id1, _ = aruco.detectMarkers(g1, aruco_dict)\n",
    "        c2, id2, _ = aruco.detectMarkers(g2, aruco_dict)\n",
    "        r1, cc1, cid1 = aruco.interpolateCornersCharuco(c1, id1, g1, charuco_board)\n",
    "        r2, cc2, cid2 = aruco.interpolateCornersCharuco(c2, id2, g2, charuco_board)\n",
    "        if r1 < 20 or r2 < 20: continue\n",
    "\n",
    "        # Build object points (same for both)\n",
    "        objp = charuco_board.chessboardCorners[cid1.flatten()]\n",
    "        objpoints.append(objp)\n",
    "        imgpts1.append(cc1)\n",
    "        imgpts2.append(cc2)\n",
    "\n",
    "    # Stereo calibrate\n",
    "    rms, _, _, _, _, R, T, E, F = cv2.stereoCalibrate(\n",
    "        objpoints, imgpts1, imgpts2,\n",
    "        K1, d1, K2, d2, size,\n",
    "        criteria=(cv2.TERM_CRITERIA_MAX_ITER|cv2.TERM_CRITERIA_EPS,100,1e-5),\n",
    "        flags=flags\n",
    "    )\n",
    "    print(f\"Stereo RMS error: {rms:.4f}\")\n",
    "    return R, T\n",
    "\n",
    "\n",
    "def camera_to_world_charuco(glob_pattern, charuco_board, aruco_dict, K, dist):\n",
    "    for fname in glob.glob(glob_pattern):\n",
    "        im = cv2.imread(fname); gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        # 1. Detect markers + ChArUco\n",
    "        markers, ids, _ = aruco.detectMarkers(gray, aruco_dict)\n",
    "        ret, cc, cid = aruco.interpolateCornersCharuco(markers, ids, gray, charuco_board)\n",
    "        if ret < 20: continue\n",
    "\n",
    "        # 2. Get the corresponding 3D object points\n",
    "        obj_pts = charuco_board.chessboardCorners[cid.flatten()]\n",
    "\n",
    "        # 3. SolvePnP\n",
    "        _, rvec, tvec = cv2.solvePnP(obj_pts, cc, K, dist)\n",
    "        # 4. (Optional) compute reprojection error\n",
    "        proj, _ = cv2.projectPoints(obj_pts, rvec, tvec, K, dist)\n",
    "        err = np.linalg.norm(cc.reshape(-1,2) - proj.reshape(-1,2)) / len(proj)\n",
    "        print(f\"Cam→World PnP error: {err*1000:.2f} mm\")\n",
    "        return rvec, tvec\n",
    "\n",
    "    raise RuntimeError(\"ChArUco board not found in any image\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define ChArUco board\n",
    "    square_size = 0.006\n",
    "    marker_length = 0.004\n",
    "    board_size = (10, 10)\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "    charuco_board = aruco.CharucoBoard(\n",
    "        size=board_size,\n",
    "        squareLength=square_size, markerLength=marker_length\n",
    "    )\n",
    "\n",
    "    # 1) Intrinsics\n",
    "    _, K1, d1, _, _ = calibrate_intrinsics(\"calib_images/cam1/*.png\")\n",
    "    _, K2, d2, _, _ = calibrate_intrinsics(\"calib_images/cam2/*.png\")\n",
    "    \n",
    "    # 2) Stereo extrinsics\n",
    "    R, T, E, F = stereo_calibrate(\n",
    "        \"calib_images/cam1/*.png\", \"calib_images/cam2/*.png\",\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K1=K1, d1=d1, K2=K2, d2=d2\n",
    "    )\n",
    "    \n",
    "    # 3) Camera-to-world\n",
    "    rvec1, tvec1 = calibrate_camera_to_world(\n",
    "        \"marker_images/cam1/*.png\", None,\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K=K1, dist=d1\n",
    "    )\n",
    "    rvec2, tvec2 = calibrate_camera_to_world(\n",
    "        \"marker_images/cam2/*.png\", None,\n",
    "        pattern_size=(9,6), square_size=0.025,\n",
    "        K=K2, dist=d2\n",
    "    )\n",
    "    \n",
    "    print(\"Calibration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785abc0b",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def back_project(coord, camera_number):\n",
    "    \"\"\"\n",
    "    Back-project a 3D point to 2D pixel coordinates in the image plane.\n",
    "    \n",
    "    Args:\n",
    "        coord: 3D point in world coordinates.\n",
    "        camera_number: 0 for left camera, 1 for right camera.\n",
    "        \n",
    "    Returns:\n",
    "        pixel_coord: 2D pixel coordinates in the image plane.\n",
    "    \"\"\"\n",
    "    # Camera intrinsics and extrinsics (example values)\n",
    "    K = np.array([[1000, 0, 320],\n",
    "                  [0, 1000, 240],\n",
    "                  [0, 0, 1]])\n",
    "    R = np.eye(3) if camera_number == 0 else np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    T = np.array([0.5, 0, 0]) if camera_number == 1 else np.array([0, 0, 0])\n",
    "    \n",
    "    # Project the point\n",
    "    coord_homogeneous = np.append(coord, 1) # Convert to homogeneous coordinates\n",
    "    pixel_coord_homogeneous = K @ (R @ coord_homogeneous + T)\n",
    "    pixel_coord = pixel_coord_homogeneous[:2] / pixel_coord_homogeneous[2]\n",
    "    \n",
    "    return pixel_coord\n",
    "\n",
    "# 1) Create lookup table for voxel grid\n",
    "Nx, Ny, Nz = 20, 20, 20\n",
    "origin = np.array([0, 0, 0])\n",
    "voxel_size = np.array([0.01, 0.01, 0.01])\n",
    "lookup_table = np.zeros((Nx, Ny, Nz, 2, 2), dtype=np.float32)\n",
    "for x in range(Nx):\n",
    "    for y in range(Ny):\n",
    "        for z in range(Nz):\n",
    "            coord = origin + np.array([x, y, z]) * voxel_size\n",
    "            for camera_number in range(2):\n",
    "                pixel_coord = back_project(coord, camera_number)\n",
    "                lookup_table[x, y, z, camera_number] = np.reshape(pixel_coord, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Obtain masks for left and right cameras\n",
    "# TODO: Replace with actual image loading and processing\n",
    "mask_0 = np.zeros((480, 640), dtype=np.uint8)\n",
    "mask_1 = np.zeros((480, 640), dtype=np.uint8)\n",
    "\n",
    "# 3) Voxel carving: keep (x,y,z) if both silhouettes agree\n",
    "volume = np.zeros((Nx, Ny, Nz), dtype=np.uint8)\n",
    "for x in range(Nx):\n",
    "    for y in range(Ny):\n",
    "        for z in range(Nz):\n",
    "            pixel_coord_0 = lookup_table[x, y, z, 0]\n",
    "            pixel_coord_1 = lookup_table[x, y, z, 1]\n",
    "            if mask_0[pixel_coord_0[0], pixel_coord_0[1]] and \\\n",
    "               mask_1[pixel_coord_1[0], pixel_coord_1[1]]:\n",
    "                volume[x, y, z] = 1\n",
    "                \n",
    "points = np.argwhere(volume == 1)  # Get indices of carved voxels\n",
    "                \n",
    "\n",
    "# 4) Compute a simple centerline: for each z-slice, average x & y\n",
    "centerline = []\n",
    "for zk in z:\n",
    "    slice_pts = points[np.isclose(points[:, 2], zk)]\n",
    "    if len(slice_pts) > 0:\n",
    "        cx, cy = slice_pts[:,0].mean(), slice_pts[:,1].mean()\n",
    "        centerline.append([cx, cy, zk])\n",
    "centerline = np.array(centerline)\n",
    "\n",
    "# 5) Estimate tangent at the tip (last two points)\n",
    "v = centerline[-1] - centerline[-2]\n",
    "theta = np.arccos(v[2] / np.linalg.norm(v))  # bending magnitude\n",
    "phi   = np.arctan2(v[1], v[0])              # bending plane\n",
    "\n",
    "print(\"Estimated bending angle θ (rad):\", theta)\n",
    "print(\"Estimated bending plane φ (rad):\", phi)\n",
    "\n",
    "# 6) Plot a 2D x–z view of the carved voxels and the centerline\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(points[:,0], points[:,2], marker='.', label='voxels')\n",
    "ax.plot(centerline[:,0], centerline[:,2], label='centerline')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('z')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0475980",
   "metadata": {},
   "source": [
    "# Create Charuco board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28669331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved accurate PDF: charuco_board_accurate.pdf\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import mm\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for the ChArUco board\n",
    "SQUARE_SIZE_MM = 6   # Desired square size in mm\n",
    "MARKER_SIZE_MM = 4   # Desired marker size in mm\n",
    "SQUARES_X = 10       # Number of squares along x-axis\n",
    "SQUARES_Y = 10       # Number of squares along y-axis\n",
    "MARGIN_MM = 10       # Margin around the board\n",
    "\n",
    "# Create the ArUco dictionary\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)\n",
    "\n",
    "# Define the board size as a tuple (number of squares in x and y directions)\n",
    "board_size = (SQUARES_X, SQUARES_Y)\n",
    "\n",
    "# Create the Charuco board using the updated constructor\n",
    "charuco_board = aruco.CharucoBoard(\n",
    "    size=board_size,\n",
    "    squareLength=SQUARE_SIZE_MM / 1000.0,  # in meters\n",
    "    markerLength=MARKER_SIZE_MM / 1000.0,  # in meters\n",
    "    dictionary=aruco_dict\n",
    ")\n",
    "\n",
    "# Generate the board image using OpenCV\n",
    "img_size = (SQUARES_X * 300, SQUARES_Y * 300)  # Higher resolution for print quality\n",
    "img = charuco_board.generateImage(img_size)\n",
    "\n",
    "# Save the board image as a PNG for reference\n",
    "cv2.imwrite(\"charuco_board.png\", img)\n",
    "\n",
    "# Calculate the final PDF size including margins\n",
    "pdf_width = (SQUARES_X * SQUARE_SIZE_MM) + (2 * MARGIN_MM)  # mm\n",
    "pdf_height = (SQUARES_Y * SQUARE_SIZE_MM) + (2 * MARGIN_MM)  # mm\n",
    "\n",
    "# Create a PDF using ReportLab with precise dimensions\n",
    "pdf_path = \"charuco_board_accurate.pdf\"\n",
    "c = canvas.Canvas(pdf_path, pagesize=(pdf_width * mm, pdf_height * mm))\n",
    "\n",
    "# Convert the OpenCV image to a format suitable for PDF\n",
    "_, buffer = cv2.imencode(\".png\", img)\n",
    "image_data = buffer.tobytes()\n",
    "\n",
    "# Calculate the exact position for centering the image\n",
    "x_offset = MARGIN_MM * mm\n",
    "y_offset = MARGIN_MM * mm\n",
    "image_width = (SQUARES_X * SQUARE_SIZE_MM) * mm\n",
    "image_height = (SQUARES_Y * SQUARE_SIZE_MM) * mm\n",
    "\n",
    "# Draw the ChArUco board image onto the PDF\n",
    "c.drawImage(\"charuco_board.png\", x_offset, y_offset, image_width, image_height)\n",
    "\n",
    "# Save the PDF\n",
    "c.showPage()\n",
    "c.save()\n",
    "print(f\"Saved accurate PDF: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b6f4b",
   "metadata": {},
   "source": [
    "# Pixel Color Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class PixelColorClassifier:\n",
    "    def __init__(self, model_type='logistic', color_space='HSV'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model_type: 'logistic' or 'naive_bayes'\n",
    "            color_space: 'HSV', 'RGB', or 'BGR'\n",
    "        \"\"\"\n",
    "        if model_type == 'logistic':\n",
    "            self.model = LogisticRegression(max_iter=200)\n",
    "        elif model_type == 'naive_bayes':\n",
    "            self.model = GaussianNB()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model_type. Choose 'logistic' or 'naive_bayes'.\")\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.color_space = color_space\n",
    "        self.trained = False\n",
    "\n",
    "    def _convert_color(self, image_bgr):\n",
    "        import cv2\n",
    "        if self.color_space == 'HSV':\n",
    "            return cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "        elif self.color_space == 'RGB':\n",
    "            return cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        elif self.color_space == 'BGR':\n",
    "            return image_bgr\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported color space\")\n",
    "\n",
    "    def fit(self, image_bgr, fg_mask, bg_mask, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Train the model using foreground and background binary masks.\n",
    "\n",
    "        Parameters:\n",
    "            image_bgr: Input image (BGR format)\n",
    "            fg_mask: Binary mask where foreground pixels are 255\n",
    "            bg_mask: Binary mask where background pixels are 255\n",
    "        \"\"\"\n",
    "        img_color = self._convert_color(image_bgr)\n",
    "        H, W, _ = img_color.shape\n",
    "        flat_img = img_color.reshape(-1, 3)\n",
    "\n",
    "        fg_indices = np.where(fg_mask.flatten() > 0)[0]\n",
    "        bg_indices = np.where(bg_mask.flatten() > 0)[0]\n",
    "\n",
    "        n_fg = min(max_samples, len(fg_indices))\n",
    "        n_bg = min(max_samples, len(bg_indices))\n",
    "\n",
    "        fg_sample = np.random.choice(fg_indices, n_fg, replace=False)\n",
    "        bg_sample = np.random.choice(bg_indices, n_bg, replace=False)\n",
    "\n",
    "        X = np.vstack((flat_img[fg_sample], flat_img[bg_sample]))\n",
    "        y = np.hstack((np.ones(n_fg), np.zeros(n_bg)))\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "        self.trained = True\n",
    "\n",
    "    def predict_proba(self, image_bgr):\n",
    "        \"\"\"\n",
    "        Predict foreground probability for each pixel.\n",
    "\n",
    "        Parameters:\n",
    "            image_bgr: Input image (BGR format)\n",
    "\n",
    "        Returns:\n",
    "            prob_map: 2D numpy array of foreground probabilities\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise RuntimeError(\"PixelColorClassifier must be trained with `fit()` before prediction.\")\n",
    "\n",
    "        img_color = self._convert_color(image_bgr)\n",
    "        H, W, _ = img_color.shape\n",
    "        flat_img = img_color.reshape(-1, 3)\n",
    "\n",
    "        proba = self.model.predict_proba(flat_img)[:, 1]\n",
    "        return proba.reshape(H, W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47629909",
   "metadata": {},
   "source": [
    "# Collect Calib Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACE to capture image, ESC to exit.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def configure_camera(devices):\n",
    "    for device in devices:\n",
    "\n",
    "        print(f\"Configuring camera on {device}...\")\n",
    "\n",
    "        # Build the commands to configure the camera\n",
    "        commands = [\n",
    "            f\"v4l2-ctl -d {device} -c focus_auto=0\",\n",
    "            f\"v4l2-ctl -d {device} -c focus_absolute=76\",\n",
    "            f\"v4l2-ctl -d {device} -c exposure_auto=1\",\n",
    "            f\"v4l2-ctl -d {device} -c exposure_absolute=333\",\n",
    "            # f\"v4l2-ctl -d {device} -c white_balance_temperature_auto=0\",\n",
    "            # f\"v4l2-ctl -d {device} -c white_balance_temperature=4000\",\n",
    "            # f\"v4l2-ctl -d {device} -c brightness=128\",\n",
    "            # f\"v4l2-ctl -d {device} -c contrast=128\",\n",
    "            # f\"v4l2-ctl -d {device} -c saturation=128\",\n",
    "        ]\n",
    "\n",
    "        for command in commands:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(\"Camera configuration complete!\")\n",
    "\n",
    "# Camera settings\n",
    "camera_id = [0]\n",
    "devices = [f\"/dev/video{i}\" for i in camera_id] \n",
    "configure_camera(devices)\n",
    "output_dir = \"calib_images/cam1\"  # Directory to save images\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(camera_id[0], cv2.CAP_V4L2)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press SPACE to capture image, ESC to exit.\")\n",
    "\n",
    "image_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "\n",
    "    # Display the live feed\n",
    "    cv2.imshow(\"Camera Feed - Press SPACE to capture, ESC to exit\", frame)\n",
    "\n",
    "    # Keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Save image on SPACE key press\n",
    "    if key == ord(' '):\n",
    "        # Construct image filename\n",
    "        image_path = os.path.join(output_dir, f\"img_{image_count:03d}.png\")\n",
    "        # Save the frame as PNG\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        print(f\"Captured: {image_path}\")\n",
    "        image_count += 1\n",
    "\n",
    "    # Exit on ESC key press\n",
    "    elif key == 27:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "# Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec951fee",
   "metadata": {},
   "source": [
    "# Find correct focus distance   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c74f0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the UP and DOWN arrow keys to adjust focus. Press ESC to exit.\n",
      "Focus set to: 125\n",
      "Focus set to: 130\n",
      "Focus set to: 135\n",
      "Focus set to: 140\n",
      "Focus set to: 145\n",
      "Focus set to: 150\n",
      "Focus set to: 155\n",
      "Focus set to: 150\n",
      "Focus set to: 145\n",
      "Focus set to: 140\n",
      "Focus set to: 135\n",
      "Focus set to: 130\n",
      "Focus set to: 125\n",
      "Focus set to: 120\n",
      "Focus set to: 115\n",
      "Focus set to: 110\n",
      "Focus set to: 105\n",
      "Focus set to: 100\n",
      "Focus set to: 95\n",
      "Focus set to: 90\n",
      "Focus set to: 85\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 70\n",
      "Focus set to: 65\n",
      "Focus set to: 60\n",
      "Focus set to: 55\n",
      "Focus set to: 50\n",
      "Focus set to: 55\n",
      "Focus set to: 60\n",
      "Focus set to: 65\n",
      "Focus set to: 70\n",
      "Focus set to: 65\n",
      "Focus set to: 70\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 85\n",
      "Focus set to: 90\n",
      "Focus set to: 85\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 70\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 70\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 85\n",
      "Focus set to: 90\n",
      "Focus set to: 85\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 70\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 70\n",
      "Focus set to: 65\n",
      "Focus set to: 60\n",
      "Focus set to: 65\n",
      "Focus set to: 70\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n",
      "Focus set to: 75\n",
      "Focus set to: 80\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "\n",
    "def set_focus(value):\n",
    "    command = f\"v4l2-ctl -d /dev/video0 -c focus_absolute={value}\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "cap.set(cv2.CAP_PROP_AUTOFOCUS, 0)  # Disable autofocus\n",
    "\n",
    "print(\"Use the UP and DOWN arrow keys to adjust focus. Press ESC to exit.\")\n",
    "focus_value = 120  # Start with a mid-range value\n",
    "set_focus(focus_value)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read from camera.\")\n",
    "        break\n",
    "\n",
    "    # Display current focus value on the frame\n",
    "    cv2.putText(frame, f\"Focus: {focus_value}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Adjust Focus - Use UP/DOWN keys\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC key to exit\n",
    "        break\n",
    "    elif key == ord('w'):  # Increase focus\n",
    "        focus_value = min(focus_value + 5, 250)\n",
    "        set_focus(focus_value)\n",
    "        print(f\"Focus set to: {focus_value}\")\n",
    "    elif key == ord('s'):  # Decrease focus\n",
    "        focus_value = max(focus_value - 5, 0)\n",
    "        set_focus(focus_value)\n",
    "        print(f\"Focus set to: {focus_value}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
